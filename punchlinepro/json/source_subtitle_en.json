{
  "events": [
    {
      "tStartMs": 5879,
      "dDurationMs": 7361,
      "segs": [
        {
          "utf8": "how would I teach a robot to tie a shoe computers are good at doing exactly"
        }
      ]
    },
    {
      "tStartMs": 13240,
      "dDurationMs": 5720,
      "segs": [
        {
          "utf8": "what you tell them to do which is both a blessing and a curse if I want to add"
        }
      ]
    },
    {
      "tStartMs": 18960,
      "dDurationMs": 4680,
      "segs": [
        {
          "utf8": "two very large numbers that's perfect the computer can do that but if I want a"
        }
      ]
    },
    {
      "tStartMs": 23640,
      "dDurationMs": 5799,
      "segs": [
        {
          "utf8": "computer or robot to do everyday tasks like tie a shoe fold a t-shirts or even"
        }
      ]
    },
    {
      "tStartMs": 29439,
      "dDurationMs": 4321,
      "segs": [
        {
          "utf8": "just walk it's impossible to specify exactly how"
        }
      ]
    },
    {
      "tStartMs": 33760,
      "dDurationMs": 5880,
      "segs": [
        {
          "utf8": "to move its joints and interact with the environment and exactly what the forces"
        }
      ]
    },
    {
      "tStartMs": 39640,
      "dDurationMs": 4680,
      "segs": [
        {
          "utf8": "masses and friction of the physical world are reinforcement learning is a"
        }
      ]
    },
    {
      "tStartMs": 44320,
      "dDurationMs": 4680,
      "segs": [
        {
          "utf8": "field of machine learning that tries to solve precisely that it's quite"
        }
      ]
    },
    {
      "tStartMs": 49000,
      "dDurationMs": 3280,
      "segs": [
        {
          "utf8": "appealing to people because of how it resembles the way that humans and"
        }
      ]
    },
    {
      "tStartMs": 52280,
      "dDurationMs": 4799,
      "segs": [
        {
          "utf8": "animals learn in the physical world through trial and error and using"
        }
      ]
    },
    {
      "tStartMs": 57079,
      "dDurationMs": 5120,
      "segs": [
        {
          "utf8": "positive and negative reward to reinforce the behaviors that you want"
        }
      ]
    },
    {
      "tStartMs": 62199,
      "dDurationMs": 4321,
      "segs": [
        {
          "utf8": "and punish the behaviors that you don't want in recent years some of the"
        }
      ]
    },
    {
      "tStartMs": 66520,
      "dDurationMs": 4520,
      "segs": [
        {
          "utf8": "achievements of reinforcement learning include learning to play Atari games in"
        }
      ]
    },
    {
      "tStartMs": 71040,
      "dDurationMs": 5719,
      "segs": [
        {
          "utf8": "2013 defeating top human go players in 2016 with Alpha go defeating"
        }
      ]
    },
    {
      "tStartMs": 76759,
      "dDurationMs": 5400,
      "segs": [
        {
          "utf8": "professional Dota 2 players in 2018 with open AI 5 solving a Rubik's Cube with a"
        }
      ]
    },
    {
      "tStartMs": 82159,
      "dDurationMs": 6561,
      "segs": [
        {
          "utf8": "robot Hands by open AI in 2019 certain applications in robot dogs self-driving"
        }
      ]
    },
    {
      "tStartMs": 88720,
      "dDurationMs": 6200,
      "segs": [
        {
          "utf8": "fine tooting language model Etc and all those AI learns to YouTube videos of"
        }
      ]
    },
    {
      "tStartMs": 94920,
      "dDurationMs": 6440,
      "segs": [
        {
          "utf8": "which I have made one as well this video is over an hour long very dense in"
        }
      ]
    },
    {
      "tStartMs": 101360,
      "dDurationMs": 5399,
      "segs": [
        {
          "utf8": "information and not very entertaining its purpose is to help those who are"
        }
      ]
    },
    {
      "tStartMs": 106759,
      "dDurationMs": 4680,
      "segs": [
        {
          "utf8": "interested learn as much of reinforcement learning as possible as"
        }
      ]
    },
    {
      "tStartMs": 111439,
      "dDurationMs": 5121,
      "segs": [
        {
          "utf8": "easily as possible and as fast as possible this is because I think that"
        }
      ]
    },
    {
      "tStartMs": 116560,
      "dDurationMs": 3839,
      "segs": [
        {
          "utf8": "this technology and field of study will be very important in the next few"
        }
      ]
    },
    {
      "tStartMs": 120399,
      "dDurationMs": 4601,
      "segs": [
        {
          "utf8": "decades when artificial intelligence gradually brings its capabilities to the"
        }
      ]
    },
    {
      "tStartMs": 125000,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "physical world and I want to make the knowledge as accessible as possible to"
        }
      ]
    },
    {
      "tStartMs": 129160,
      "dDurationMs": 4680,
      "segs": [
        {
          "utf8": "all people we will go over much of the foundational Theory and math while"
        }
      ]
    },
    {
      "tStartMs": 133840,
      "dDurationMs": 4320,
      "segs": [
        {
          "utf8": "demonstrating it in practice with a series of example tasks that gradually"
        }
      ]
    },
    {
      "tStartMs": 138160,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "increase in complexity that will be the bulk of the video and then following"
        }
      ]
    },
    {
      "tStartMs": 143200,
      "dDurationMs": 3440,
      "segs": [
        {
          "utf8": "that we will cover the interesting correlations between reinforcement"
        }
      ]
    },
    {
      "tStartMs": 146640,
      "dDurationMs": 3840,
      "segs": [
        {
          "utf8": "learning and Neuroscience and how the brain works"
        }
      ]
    },
    {
      "tStartMs": 150480,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "and then we will talk about the aspects in which reinforcement learning is still"
        }
      ]
    },
    {
      "tStartMs": 154640,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "lacking and some of the directions of future research that I think will be of"
        }
      ]
    },
    {
      "tStartMs": 159360,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "great help this video is not meant to be watched just once so feel free to"
        }
      ]
    },
    {
      "tStartMs": 164640,
      "dDurationMs": 5120,
      "segs": [
        {
          "utf8": "re-watch it as many times as you need to extract its value the prerequisite"
        }
      ]
    },
    {
      "tStartMs": 169760,
      "dDurationMs": 5600,
      "segs": [
        {
          "utf8": "knowledge of this video includes maths up until basic calculus and a basic"
        }
      ]
    },
    {
      "tStartMs": 175360,
      "dDurationMs": 4320,
      "segs": [
        {
          "utf8": "understanding of neural networks in order to research for this video some of"
        }
      ]
    },
    {
      "tStartMs": 179680,
      "dDurationMs": 5080,
      "segs": [
        {
          "utf8": "the main sources of information I used include the book reinforcement learning"
        }
      ]
    },
    {
      "tStartMs": 184760,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "and introduction by suton and Barto which is widely regarded as the Holy"
        }
      ]
    },
    {
      "tStartMs": 189560,
      "dDurationMs": 4759,
      "segs": [
        {
          "utf8": "Bible of reinforcement learning the open AI spinning up website which has"
        }
      ]
    },
    {
      "tStartMs": 194319,
      "dDurationMs": 4361,
      "segs": [
        {
          "utf8": "compiled a lot of information about the most famous and influential algorithms"
        }
      ]
    },
    {
      "tStartMs": 198680,
      "dDurationMs": 5160,
      "segs": [
        {
          "utf8": "large language models such as chat gbt which are really good at searching for"
        }
      ]
    },
    {
      "tStartMs": 203840,
      "dDurationMs": 5399,
      "segs": [
        {
          "utf8": "compiling and summarizing widely known knowledge and the reinforcement learning"
        }
      ]
    },
    {
      "tStartMs": 209239,
      "dDurationMs": 2200,
      "segs": [
        {
          "utf8": "subreddit where you can read through other"
        }
      ]
    },
    {
      "tStartMs": 211439,
      "dDurationMs": 4121,
      "segs": [
        {
          "utf8": "people's questions and answers on the topic as well as these resources I'll"
        }
      ]
    },
    {
      "tStartMs": 215560,
      "dDurationMs": 3720,
      "segs": [
        {
          "utf8": "leave a link to some of the interesting articles that I mention in this video in"
        }
      ]
    },
    {
      "tStartMs": 219280,
      "dDurationMs": 5080,
      "segs": [
        {
          "utf8": "the description now before we begin have a look at my hair I've chosen to film"
        }
      ]
    },
    {
      "tStartMs": 224360,
      "dDurationMs": 4200,
      "segs": [
        {
          "utf8": "the introduction at the very end and throughout the different segments of the"
        }
      ]
    },
    {
      "tStartMs": 228560,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "video you'll notice that my hair has grown quite a bit indeed this video was"
        }
      ]
    },
    {
      "tStartMs": 233280,
      "dDurationMs": 4360,
      "segs": [
        {
          "utf8": "approximately half a year in the making with even just the filming spanning"
        }
      ]
    },
    {
      "tStartMs": 237640,
      "dDurationMs": 4239,
      "segs": [
        {
          "utf8": "several months and no I didn't avoid haircuts just to prove a point I was"
        }
      ]
    },
    {
      "tStartMs": 241879,
      "dDurationMs": 3881,
      "segs": [
        {
          "utf8": "just lazy but if you watch through the video and find Value in it consider"
        }
      ]
    },
    {
      "tStartMs": 245760,
      "dDurationMs": 4559,
      "segs": [
        {
          "utf8": "supporting the channel on patreon link in description I've put some of the"
        }
      ]
    },
    {
      "tStartMs": 250319,
      "dDurationMs": 4681,
      "segs": [
        {
          "utf8": "behind the scenes demo videos on there as of now but those are in this video"
        }
      ]
    },
    {
      "tStartMs": 255000,
      "dDurationMs": 4880,
      "segs": [
        {
          "utf8": "anyways so consider supporting if you're interested in topics related to Ai and"
        }
      ]
    },
    {
      "tStartMs": 259880,
      "dDurationMs": 4080,
      "segs": [
        {
          "utf8": "Robotics and you'd like to see behind the scenes of future videos with that"
        }
      ]
    },
    {
      "tStartMs": 263960,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "being said let's get started"
        }
      ]
    },
    {
      "tStartMs": 269400,
      "dDurationMs": 3090,
      "segs": [
        {
          "utf8": "[Music]"
        }
      ]
    },
    {
      "tStartMs": 273720,
      "dDurationMs": 5919,
      "segs": [
        {
          "utf8": "in reinforcement learning which I'll refer to in the rest of the video as RL"
        }
      ]
    },
    {
      "tStartMs": 279639,
      "dDurationMs": 6521,
      "segs": [
        {
          "utf8": "there is an agent and an environment agent is the car environment is the road"
        }
      ]
    },
    {
      "tStartMs": 286160,
      "dDurationMs": 4599,
      "segs": [
        {
          "utf8": "agent is the player environment is the map agent is the hand environment is a"
        }
      ]
    },
    {
      "tStartMs": 290759,
      "dDurationMs": 5481,
      "segs": [
        {
          "utf8": "cube and agent is the strategy environment is the chess match the"
        }
      ]
    },
    {
      "tStartMs": 296240,
      "dDurationMs": 4959,
      "segs": [
        {
          "utf8": "definition of agents and environments which generalize to all these situations"
        }
      ]
    },
    {
      "tStartMs": 301199,
      "dDurationMs": 6681,
      "segs": [
        {
          "utf8": "is that we can say the agent is whatever you can directly control and the"
        }
      ]
    },
    {
      "tStartMs": 307880,
      "dDurationMs": 4599,
      "segs": [
        {
          "utf8": "environment is whatever you cannot directly control but you're trying to"
        }
      ]
    },
    {
      "tStartMs": 312479,
      "dDurationMs": 5881,
      "segs": [
        {
          "utf8": "indirectly interact with through the agent now when the agent interacts with"
        }
      ]
    },
    {
      "tStartMs": 318360,
      "dDurationMs": 5399,
      "segs": [
        {
          "utf8": "the environment obviously there is two directions of influence influence of the"
        }
      ]
    },
    {
      "tStartMs": 323759,
      "dDurationMs": 4961,
      "segs": [
        {
          "utf8": "agent on the environment is called an action and as a human you can"
        }
      ]
    },
    {
      "tStartMs": 328720,
      "dDurationMs": 5000,
      "segs": [
        {
          "utf8": "intuitively think of the movements in your limbs and your muscles influence of"
        }
      ]
    },
    {
      "tStartMs": 333720,
      "dDurationMs": 5000,
      "segs": [
        {
          "utf8": "the environments on the agents is called State and as a human it's your sensory"
        }
      ]
    },
    {
      "tStartMs": 338720,
      "dDurationMs": 6479,
      "segs": [
        {
          "utf8": "input you're observing the state of the environment as a human the sensory input"
        }
      ]
    },
    {
      "tStartMs": 345199,
      "dDurationMs": 5000,
      "segs": [
        {
          "utf8": "and motor output Loop is kind of like a form of this agent environment"
        }
      ]
    },
    {
      "tStartMs": 350199,
      "dDurationMs": 5041,
      "segs": [
        {
          "utf8": "interaction where the brain is the agent responsible for integrating data and"
        }
      ]
    },
    {
      "tStartMs": 355240,
      "dDurationMs": 5200,
      "segs": [
        {
          "utf8": "decision- making in RL we're trying to create such an agent such a brain using"
        }
      ]
    },
    {
      "tStartMs": 360440,
      "dDurationMs": 4520,
      "segs": [
        {
          "utf8": "a computer so of course all these states and actions have to be represented as"
        }
      ]
    },
    {
      "tStartMs": 364960,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "numbers the state could be position and velocity colors of pixels temperature"
        }
      ]
    },
    {
      "tStartMs": 370960,
      "dDurationMs": 5160,
      "segs": [
        {
          "utf8": "battery level Etc and the action could include numbers representing how fast to"
        }
      ]
    },
    {
      "tStartMs": 376120,
      "dDurationMs": 6079,
      "segs": [
        {
          "utf8": "spin a motor which direction to go which ability to activate in a video game Etc"
        }
      ]
    },
    {
      "tStartMs": 382199,
      "dDurationMs": 4801,
      "segs": [
        {
          "utf8": "now remember where exactly you draw the boundary between agent and environment"
        }
      ]
    },
    {
      "tStartMs": 387000,
      "dDurationMs": 3280,
      "segs": [
        {
          "utf8": "is actually more nuanced than you might think"
        }
      ]
    },
    {
      "tStartMs": 390280,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "and it depends entirely on your realm of control and what specific task you're"
        }
      ]
    },
    {
      "tStartMs": 395000,
      "dDurationMs": 5639,
      "segs": [
        {
          "utf8": "trying to achieve only what you can directly control what you can take for"
        }
      ]
    },
    {
      "tStartMs": 400639,
      "dDurationMs": 6761,
      "segs": [
        {
          "utf8": "granted is the agent for example imagine I'm learning how to drive a car but I'm"
        }
      ]
    },
    {
      "tStartMs": 407400,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "starting from zero I don't even know how to move my limbs yet we need to write a"
        }
      ]
    },
    {
      "tStartMs": 412840,
      "dDurationMs": 7160,
      "segs": [
        {
          "utf8": "program let's call it program a and put it in my brain in order to learn how to"
        }
      ]
    },
    {
      "tStartMs": 420000,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "move my limbs here what we can directly control are the neuron signals that my"
        }
      ]
    },
    {
      "tStartMs": 425440,
      "dDurationMs": 5800,
      "segs": [
        {
          "utf8": "brain sends down my spinal cord into my muscles what we cannot directly control"
        }
      ]
    },
    {
      "tStartMs": 431240,
      "dDurationMs": 5840,
      "segs": [
        {
          "utf8": "but are trying to influence indirectly are the movements of my limbs so we can"
        }
      ]
    },
    {
      "tStartMs": 437080,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "say the agent is my brain the environment is my limbs the actions are"
        }
      ]
    },
    {
      "tStartMs": 442120,
      "dDurationMs": 6440,
      "segs": [
        {
          "utf8": "my neuron signals and the state includes the positions joint angles and movements"
        }
      ]
    },
    {
      "tStartMs": 448560,
      "dDurationMs": 5560,
      "segs": [
        {
          "utf8": "of my limbs as observed through Visual feedback and proprioception now pretend"
        }
      ]
    },
    {
      "tStartMs": 454120,
      "dDurationMs": 3720,
      "segs": [
        {
          "utf8": "we've fine-tuned this program enough so now I'm pretty confident with limb"
        }
      ]
    },
    {
      "tStartMs": 457840,
      "dDurationMs": 5720,
      "segs": [
        {
          "utf8": "movement in my forearm for example I can open and close my grip do some wrist"
        }
      ]
    },
    {
      "tStartMs": 463560,
      "dDurationMs": 3680,
      "segs": [
        {
          "utf8": "flexion and extension and occasionally if I'm"
        }
      ]
    },
    {
      "tStartMs": 467240,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "feeling extra adventurous maybe even dabble in some allar and Radial"
        }
      ]
    },
    {
      "tStartMs": 472280,
      "dDurationMs": 6039,
      "segs": [
        {
          "utf8": "deviation now we have to write program B to control how I'll interact with the"
        }
      ]
    },
    {
      "tStartMs": 478319,
      "dDurationMs": 6160,
      "segs": [
        {
          "utf8": "car the agent is me as a living creature the environment is the car the actions"
        }
      ]
    },
    {
      "tStartMs": 484479,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "are my limb movements and the state consists of the elements of the car"
        }
      ]
    },
    {
      "tStartMs": 489039,
      "dDurationMs": 4401,
      "segs": [
        {
          "utf8": "which I can observe visually such as looking at the speedometer or through"
        }
      ]
    },
    {
      "tStartMs": 493440,
      "dDurationMs": 4599,
      "segs": [
        {
          "utf8": "tactile feedback like gripping the steering wheel or stepping on the brakes"
        }
      ]
    },
    {
      "tStartMs": 498039,
      "dDurationMs": 5761,
      "segs": [
        {
          "utf8": "now that I know how to operate the car I have to learn to drive using program C"
        }
      ]
    },
    {
      "tStartMs": 503800,
      "dDurationMs": 4519,
      "segs": [
        {
          "utf8": "which let's say can now directly control how the car moves forwards and backwards"
        }
      ]
    },
    {
      "tStartMs": 508319,
      "dDurationMs": 4001,
      "segs": [
        {
          "utf8": "and how it turns now the agents is the whole car the"
        }
      ]
    },
    {
      "tStartMs": 512320,
      "dDurationMs": 4920,
      "segs": [
        {
          "utf8": "environment is the road the actions are just how the car moves and the state"
        }
      ]
    },
    {
      "tStartMs": 517240,
      "dDurationMs": 4440,
      "segs": [
        {
          "utf8": "includes other cars and their positions how many lanes there are what the speed"
        }
      ]
    },
    {
      "tStartMs": 521680,
      "dDurationMs": 6640,
      "segs": [
        {
          "utf8": "limit is ETC which I observe visually so in this agent environment model the"
        }
      ]
    },
    {
      "tStartMs": 528320,
      "dDurationMs": 4880,
      "segs": [
        {
          "utf8": "boundary between agents and environment can be quite arbitrary and the time"
        }
      ]
    },
    {
      "tStartMs": 533200,
      "dDurationMs": 5759,
      "segs": [
        {
          "utf8": "scale for these actions and state observations can also be quite arbitrary"
        }
      ]
    },
    {
      "tStartMs": 538959,
      "dDurationMs": 5841,
      "segs": [
        {
          "utf8": "like for example example if I close my fist is that one action or is it five"
        }
      ]
    },
    {
      "tStartMs": 544800,
      "dDurationMs": 5080,
      "segs": [
        {
          "utf8": "actions one for each finger the good thing about this arbitrariness is"
        }
      ]
    },
    {
      "tStartMs": 549880,
      "dDurationMs": 4639,
      "segs": [
        {
          "utf8": "because you can frame a problem in so many different ways you can choose to"
        }
      ]
    },
    {
      "tStartMs": 554519,
      "dDurationMs": 5401,
      "segs": [
        {
          "utf8": "frame it in a way that gives the problem structure and makes it easier to solve"
        }
      ]
    },
    {
      "tStartMs": 559920,
      "dDurationMs": 3159,
      "segs": [
        {
          "utf8": "so let's go over the way that reinforcement learning frames a task in"
        }
      ]
    },
    {
      "tStartMs": 563079,
      "dDurationMs": 4921,
      "segs": [
        {
          "utf8": "order to solve it which is called the marov decision process so first of all"
        }
      ]
    },
    {
      "tStartMs": 568000,
      "dDurationMs": 6160,
      "segs": [
        {
          "utf8": "there's also this added signal known as a reward which is just a single number"
        }
      ]
    },
    {
      "tStartMs": 574160,
      "dDurationMs": 3720,
      "segs": [
        {
          "utf8": "that comes after each action that represents whether that action had a"
        }
      ]
    },
    {
      "tStartMs": 577880,
      "dDurationMs": 3880,
      "segs": [
        {
          "utf8": "good or bad outcome the more positive the reward the better and the more"
        }
      ]
    },
    {
      "tStartMs": 581760,
      "dDurationMs": 4040,
      "segs": [
        {
          "utf8": "negative the reward the worse now it's up to you to interpret whether the"
        }
      ]
    },
    {
      "tStartMs": 585800,
      "dDurationMs": 4279,
      "segs": [
        {
          "utf8": "reward comes from the agents or the environment if it's a human learning"
        }
      ]
    },
    {
      "tStartMs": 590079,
      "dDurationMs": 4361,
      "segs": [
        {
          "utf8": "something we can generally decide for ourselves whether we did good or bad at"
        }
      ]
    },
    {
      "tStartMs": 594440,
      "dDurationMs": 4440,
      "segs": [
        {
          "utf8": "that task but if you're training a dog then the reward of food seems to be"
        }
      ]
    },
    {
      "tStartMs": 598880,
      "dDurationMs": 4639,
      "segs": [
        {
          "utf8": "provided externally conventionally the reward is considered to be a signal from"
        }
      ]
    },
    {
      "tStartMs": 603519,
      "dDurationMs": 4721,
      "segs": [
        {
          "utf8": "the environment to the agent so let's stick with that so now let's write out"
        }
      ]
    },
    {
      "tStartMs": 608240,
      "dDurationMs": 7480,
      "segs": [
        {
          "utf8": "what a mark of decision process is also known as an mdp it's a discret repeating"
        }
      ]
    },
    {
      "tStartMs": 615720,
      "dDurationMs": 6440,
      "segs": [
        {
          "utf8": "one after the other sequence of State action reward State action reward State"
        }
      ]
    },
    {
      "tStartMs": 622160,
      "dDurationMs": 5359,
      "segs": [
        {
          "utf8": "action reward and so on this can either go on forever or it can terminate like"
        }
      ]
    },
    {
      "tStartMs": 627519,
      "dDurationMs": 3880,
      "segs": [
        {
          "utf8": "if you're playing a level in a video game and you die on that level in which"
        }
      ]
    },
    {
      "tStartMs": 631399,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "case it's called an episode and the episode terminates just a small note on"
        }
      ]
    },
    {
      "tStartMs": 637399,
      "dDurationMs": 5240,
      "segs": [
        {
          "utf8": "the notation here I've chosen to have the time step of the reward take on the"
        }
      ]
    },
    {
      "tStartMs": 642639,
      "dDurationMs": 8841,
      "segs": [
        {
          "utf8": "time step of the state and action before it so it would go s0 a0 r0 S1 A1 R1 Etc"
        }
      ]
    },
    {
      "tStartMs": 651480,
      "dDurationMs": 4359,
      "segs": [
        {
          "utf8": "this is because I thought it makes more sense this way however I found out that"
        }
      ]
    },
    {
      "tStartMs": 655839,
      "dDurationMs": 3800,
      "segs": [
        {
          "utf8": "in other sources about reinforcement learning usually they have a so the"
        }
      ]
    },
    {
      "tStartMs": 659639,
      "dDurationMs": 6081,
      "segs": [
        {
          "utf8": "rewards time step is the same as the state and action after it so it goes s0"
        }
      ]
    },
    {
      "tStartMs": 665720,
      "dDurationMs": 7000,
      "segs": [
        {
          "utf8": "a0 R1 S1 A1 R2 Etc this is not a huge deal but as a result in some of the"
        }
      ]
    },
    {
      "tStartMs": 672720,
      "dDurationMs": 4520,
      "segs": [
        {
          "utf8": "later mathematical formulas you might see that it's a bit different compared"
        }
      ]
    },
    {
      "tStartMs": 677240,
      "dDurationMs": 4599,
      "segs": [
        {
          "utf8": "to those formulas in other sources where the time step of the reward is off by"
        }
      ]
    },
    {
      "tStartMs": 681839,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "one other sources also often choose to use uppercase letters like capital r but"
        }
      ]
    },
    {
      "tStartMs": 687839,
      "dDurationMs": 4401,
      "segs": [
        {
          "utf8": "here I've chosen to use lower case are once again not a big deal let's get on"
        }
      ]
    },
    {
      "tStartMs": 692240,
      "dDurationMs": 4599,
      "segs": [
        {
          "utf8": "with the video ideally this sequence must follow a rule called the Markov"
        }
      ]
    },
    {
      "tStartMs": 696839,
      "dDurationMs": 4401,
      "segs": [
        {
          "utf8": "property which means that each state is only dependent on its immediately"
        }
      ]
    },
    {
      "tStartMs": 701240,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "previous state and not any other state for example if you're making a robot to"
        }
      ]
    },
    {
      "tStartMs": 706680,
      "dDurationMs": 4680,
      "segs": [
        {
          "utf8": "catch a ball and you've designed it so that the state information it receives"
        }
      ]
    },
    {
      "tStartMs": 711360,
      "dDurationMs": 5279,
      "segs": [
        {
          "utf8": "only includes its position then it's unclear what the next state will be"
        }
      ]
    },
    {
      "tStartMs": 716639,
      "dDurationMs": 5161,
      "segs": [
        {
          "utf8": "because it's unclear in which direction and how fast the ball is moving you"
        }
      ]
    },
    {
      "tStartMs": 721800,
      "dDurationMs": 4240,
      "segs": [
        {
          "utf8": "would have to look at multiple consecutive states to deduce that"
        }
      ]
    },
    {
      "tStartMs": 726040,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "information if instead you also included the Velocity in the state information"
        }
      ]
    },
    {
      "tStartMs": 731480,
      "dDurationMs": 3840,
      "segs": [
        {
          "utf8": "then it would follow the mark of property and each state is only"
        }
      ]
    },
    {
      "tStartMs": 735320,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "determined by the immediately previous one of course the robot isn't"
        }
      ]
    },
    {
      "tStartMs": 739480,
      "dDurationMs": 4919,
      "segs": [
        {
          "utf8": "necessarily going to know how gravity works or how one state transitions to"
        }
      ]
    },
    {
      "tStartMs": 744399,
      "dDurationMs": 5641,
      "segs": [
        {
          "utf8": "the next but at least there's sufficient information for it to learn that now"
        }
      ]
    },
    {
      "tStartMs": 750040,
      "dDurationMs": 5799,
      "segs": [
        {
          "utf8": "mdps are they perfect do they apply to every single situation probably not"
        }
      ]
    },
    {
      "tStartMs": 755839,
      "dDurationMs": 4240,
      "segs": [
        {
          "utf8": "first of all lots of RL methods rely on this Mark of property so that they can"
        }
      ]
    },
    {
      "tStartMs": 760079,
      "dDurationMs": 4521,
      "segs": [
        {
          "utf8": "just go off of the most recent state to decide on the next action and keep it"
        }
      ]
    },
    {
      "tStartMs": 764600,
      "dDurationMs": 3560,
      "segs": [
        {
          "utf8": "simple but obviously not all situations are going to satisfy this Mark of"
        }
      ]
    },
    {
      "tStartMs": 768160,
      "dDurationMs": 4479,
      "segs": [
        {
          "utf8": "property and if we have certain states that depend on more previous States then"
        }
      ]
    },
    {
      "tStartMs": 772639,
      "dDurationMs": 3760,
      "segs": [
        {
          "utf8": "the tools that we develop using this framework aren't going to work as well"
        }
      ]
    },
    {
      "tStartMs": 776399,
      "dDurationMs": 3841,
      "segs": [
        {
          "utf8": "also with a simple sequence like this it's hard to deal with varying time"
        }
      ]
    },
    {
      "tStartMs": 780240,
      "dDurationMs": 4520,
      "segs": [
        {
          "utf8": "scales if you want to work on the scale of individual muscle movements in one"
        }
      ]
    },
    {
      "tStartMs": 784760,
      "dDurationMs": 5519,
      "segs": [
        {
          "utf8": "moment and then in the next moment zoom out to let's say which city to drive to"
        }
      ]
    },
    {
      "tStartMs": 790279,
      "dDurationMs": 3881,
      "segs": [
        {
          "utf8": "then you'd probably need a more complex framework than that another thing is"
        }
      ]
    },
    {
      "tStartMs": 794160,
      "dDurationMs": 3880,
      "segs": [
        {
          "utf8": "there's only one single number as a reward signal if you're doing tasks that"
        }
      ]
    },
    {
      "tStartMs": 798040,
      "dDurationMs": 5159,
      "segs": [
        {
          "utf8": "are more nuanced and can be evaluated in multiple aspects then maybe having only"
        }
      ]
    },
    {
      "tStartMs": 803199,
      "dDurationMs": 5401,
      "segs": [
        {
          "utf8": "a single number as a feedback for each action can be a bit limiting but this is"
        }
      ]
    },
    {
      "tStartMs": 808600,
      "dDurationMs": 3720,
      "segs": [
        {
          "utf8": "pretty much the the best that we've been able to come up with so far and"
        }
      ]
    },
    {
      "tStartMs": 812320,
      "dDurationMs": 4959,
      "segs": [
        {
          "utf8": "basically all of reinforcement learning is built on the foundation of mdps maybe"
        }
      ]
    },
    {
      "tStartMs": 817279,
      "dDurationMs": 3761,
      "segs": [
        {
          "utf8": "in the future someone could figure out a way that works better maybe that could"
        }
      ]
    },
    {
      "tStartMs": 821040,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "be you the viewer but anyways let's talk about some of the math in an mdp within"
        }
      ]
    },
    {
      "tStartMs": 826480,
      "dDurationMs": 6320,
      "segs": [
        {
          "utf8": "an mdp the ultimate goal of the agent is to figure out how to for any given time"
        }
      ]
    },
    {
      "tStartMs": 832800,
      "dDurationMs": 6399,
      "segs": [
        {
          "utf8": "step use an input state to decide on an output action in a way that Max"
        }
      ]
    },
    {
      "tStartMs": 839199,
      "dDurationMs": 6361,
      "segs": [
        {
          "utf8": "maximizes the total subsequent reward over time so let's cover these two"
        }
      ]
    },
    {
      "tStartMs": 845560,
      "dDurationMs": 4120,
      "segs": [
        {
          "utf8": "mathematical constructs the strategy that takes a state and decides on an"
        }
      ]
    },
    {
      "tStartMs": 849680,
      "dDurationMs": 5519,
      "segs": [
        {
          "utf8": "action and the cumulative reward over time the strategy we call that the"
        }
      ]
    },
    {
      "tStartMs": 855199,
      "dDurationMs": 5560,
      "segs": [
        {
          "utf8": "policy function represented by the symbol pi you can intuitively think of"
        }
      ]
    },
    {
      "tStartMs": 860759,
      "dDurationMs": 6241,
      "segs": [
        {
          "utf8": "it as a mathematical function that takes a state as an input and gives an action"
        }
      ]
    },
    {
      "tStartMs": 867000,
      "dDurationMs": 5120,
      "segs": [
        {
          "utf8": "as an output but technically the notation for this policy function is"
        }
      ]
    },
    {
      "tStartMs": 872120,
      "dDurationMs": 4959,
      "segs": [
        {
          "utf8": "usually written a bit differently to allow it to have some Randomness as in"
        }
      ]
    },
    {
      "tStartMs": 877079,
      "dDurationMs": 4120,
      "segs": [
        {
          "utf8": "given the same state the policy can produce a range of different actions"
        }
      ]
    },
    {
      "tStartMs": 881199,
      "dDurationMs": 4000,
      "segs": [
        {
          "utf8": "each with their own probability the randomness is important for exploring"
        }
      ]
    },
    {
      "tStartMs": 885199,
      "dDurationMs": 4681,
      "segs": [
        {
          "utf8": "different strategies so RL people usually write the policy function as"
        }
      ]
    },
    {
      "tStartMs": 889880,
      "dDurationMs": 6240,
      "segs": [
        {
          "utf8": "giving the probability of picking a certain action given this vertical bar"
        }
      ]
    },
    {
      "tStartMs": 896120,
      "dDurationMs": 5560,
      "segs": [
        {
          "utf8": "is a conditional given that you're in a certain State now the next thing the"
        }
      ]
    },
    {
      "tStartMs": 901680,
      "dDurationMs": 6680,
      "segs": [
        {
          "utf8": "total cumulative reward is called the return represented by capital G with a"
        }
      ]
    },
    {
      "tStartMs": 908360,
      "dDurationMs": 6240,
      "segs": [
        {
          "utf8": "subscript T the subscript T represents which time step it's for so for example"
        }
      ]
    },
    {
      "tStartMs": 914600,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "if we're talking about the sum of rewards starting from R1 then that would"
        }
      ]
    },
    {
      "tStartMs": 919160,
      "dDurationMs": 6560,
      "segs": [
        {
          "utf8": "be G1 this is just all the rewards added up starting from time step T and"
        }
      ]
    },
    {
      "tStartMs": 925720,
      "dDurationMs": 5400,
      "segs": [
        {
          "utf8": "progressively multiplied by this discount Factor gamma in increasing"
        }
      ]
    },
    {
      "tStartMs": 931120,
      "dDurationMs": 4480,
      "segs": [
        {
          "utf8": "Powers now gamma is just a number between Z and one and this can be"
        }
      ]
    },
    {
      "tStartMs": 935600,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "lowered to make rewards in the future less significant perhaps because the"
        }
      ]
    },
    {
      "tStartMs": 940160,
      "dDurationMs": 4840,
      "segs": [
        {
          "utf8": "future is inherently harder to predict so maybe you'd want that to factor less"
        }
      ]
    },
    {
      "tStartMs": 945000,
      "dDurationMs": 4839,
      "segs": [
        {
          "utf8": "into your decision- making or gamma can just be set to one in which case this"
        }
      ]
    },
    {
      "tStartMs": 949839,
      "dDurationMs": 5281,
      "segs": [
        {
          "utf8": "would be an undiscounted sum but then it would only work for terminating episodes"
        }
      ]
    },
    {
      "tStartMs": 955120,
      "dDurationMs": 3839,
      "segs": [
        {
          "utf8": "or else it'll just add up into Infinity of course it's better to try tox"
        }
      ]
    },
    {
      "tStartMs": 958959,
      "dDurationMs": 4000,
      "segs": [
        {
          "utf8": "maximize the total reward over time instead of just the immediate reward for"
        }
      ]
    },
    {
      "tStartMs": 962959,
      "dDurationMs": 5041,
      "segs": [
        {
          "utf8": "a single time step because what gets you more reward immediately might make you"
        }
      ]
    },
    {
      "tStartMs": 968000,
      "dDurationMs": 5199,
      "segs": [
        {
          "utf8": "get less reward later on kind of like delayed gratification so remember how"
        }
      ]
    },
    {
      "tStartMs": 973199,
      "dDurationMs": 4721,
      "segs": [
        {
          "utf8": "the goal is to for any given time step use an input state to determine an"
        }
      ]
    },
    {
      "tStartMs": 977920,
      "dDurationMs": 5320,
      "segs": [
        {
          "utf8": "output action in order to maximize the total subsequent reward over time using"
        }
      ]
    },
    {
      "tStartMs": 983240,
      "dDurationMs": 5320,
      "segs": [
        {
          "utf8": "this mathematical terminology now we can instead say that the goal is to find the"
        }
      ]
    },
    {
      "tStartMs": 988560,
      "dDurationMs": 4079,
      "segs": [
        {
          "utf8": "policy which maximizes return find the policy"
        }
      ]
    },
    {
      "tStartMs": 992639,
      "dDurationMs": 3161,
      "segs": [
        {
          "utf8": "which maximizes"
        }
      ]
    },
    {
      "tStartMs": 998200,
      "dDurationMs": 3089,
      "segs": [
        {
          "utf8": "[Music]"
        }
      ]
    },
    {
      "tStartMs": 1001720,
      "dDurationMs": 5160,
      "segs": [
        {
          "utf8": "return all right let's try applying this to a practical example usually when"
        }
      ]
    },
    {
      "tStartMs": 1006880,
      "dDurationMs": 4759,
      "segs": [
        {
          "utf8": "you're teaching RL you start with some sort of task involving a grid and we're"
        }
      ]
    },
    {
      "tStartMs": 1011639,
      "dDurationMs": 4000,
      "segs": [
        {
          "utf8": "going to do that too the reason is that even though no real life problem will"
        }
      ]
    },
    {
      "tStartMs": 1015639,
      "dDurationMs": 6320,
      "segs": [
        {
          "utf8": "ever be this simple in a grid each State corresponds to a single Square so it's"
        }
      ]
    },
    {
      "tStartMs": 1021959,
      "dDurationMs": 4761,
      "segs": [
        {
          "utf8": "really easy to visualize and demonstrate some key Concepts like the exploration"
        }
      ]
    },
    {
      "tStartMs": 1026720,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "exploitation trade-off temporal difference the credit assignment problem"
        }
      ]
    },
    {
      "tStartMs": 1032000,
      "dDurationMs": 5559,
      "segs": [
        {
          "utf8": "World models and Sample efficiency using our simple grid demo we're going to go"
        }
      ]
    },
    {
      "tStartMs": 1037559,
      "dDurationMs": 4201,
      "segs": [
        {
          "utf8": "through what each of these terms mean how they relate to each other and why"
        }
      ]
    },
    {
      "tStartMs": 1041760,
      "dDurationMs": 4520,
      "segs": [
        {
          "utf8": "they're important for reinforcement learning we have a maze well a really"
        }
      ]
    },
    {
      "tStartMs": 1046280,
      "dDurationMs": 4880,
      "segs": [
        {
          "utf8": "simple one with just one wall but let's still call at a maze where the agent"
        }
      ]
    },
    {
      "tStartMs": 1051160,
      "dDurationMs": 5160,
      "segs": [
        {
          "utf8": "starts on a certain cell and must get to the Target cell by moving up down left"
        }
      ]
    },
    {
      "tStartMs": 1056320,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "or right and it cannot move through walls the maximum episode length is 20"
        }
      ]
    },
    {
      "tStartMs": 1061760,
      "dDurationMs": 4400,
      "segs": [
        {
          "utf8": "so the episode terminates when it either reaches the Target or when it's been"
        }
      ]
    },
    {
      "tStartMs": 1066160,
      "dDurationMs": 3600,
      "segs": [
        {
          "utf8": "going for 20 steps already and still hasn't reached the target let's start"
        }
      ]
    },
    {
      "tStartMs": 1069760,
      "dDurationMs": 3840,
      "segs": [
        {
          "utf8": "thinking about how we can apply the theory we've developed so far and frame"
        }
      ]
    },
    {
      "tStartMs": 1073600,
      "dDurationMs": 4959,
      "segs": [
        {
          "utf8": "this task as a mark of decision process so first we'll need to define a state"
        }
      ]
    },
    {
      "tStartMs": 1078559,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "action action and reward so each state will be two numbers representing the XY"
        }
      ]
    },
    {
      "tStartMs": 1083919,
      "dDurationMs": 4841,
      "segs": [
        {
          "utf8": "coordinates that the agent is on each action will be a single number between"
        }
      ]
    },
    {
      "tStartMs": 1088760,
      "dDurationMs": 5240,
      "segs": [
        {
          "utf8": "zero and three representing either up down left or right the reward after each"
        }
      ]
    },
    {
      "tStartMs": 1094000,
      "dDurationMs": 5600,
      "segs": [
        {
          "utf8": "action will be zero if the agent enters the target cell or minus one otherwise"
        }
      ]
    },
    {
      "tStartMs": 1099600,
      "dDurationMs": 4680,
      "segs": [
        {
          "utf8": "which makes it so that the goal is to arrive at the Target as fast as possible"
        }
      ]
    },
    {
      "tStartMs": 1104280,
      "dDurationMs": 5080,
      "segs": [
        {
          "utf8": "so here we can talk about our first concept which is World models"
        }
      ]
    },
    {
      "tStartMs": 1109360,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "as humans we can obviously see the whole maze at a glance and immediately figure"
        }
      ]
    },
    {
      "tStartMs": 1114000,
      "dDurationMs": 4200,
      "segs": [
        {
          "utf8": "out how to solve it because we understand that these states represent"
        }
      ]
    },
    {
      "tStartMs": 1118200,
      "dDurationMs": 4719,
      "segs": [
        {
          "utf8": "coordinates the actions represent directions and how the reward works we"
        }
      ]
    },
    {
      "tStartMs": 1122919,
      "dDurationMs": 4000,
      "segs": [
        {
          "utf8": "know that if you're in the state of 0 0 and you pick the action of three"
        }
      ]
    },
    {
      "tStartMs": 1126919,
      "dDurationMs": 6201,
      "segs": [
        {
          "utf8": "representing right that you'll move into one Zer and get a reward of minus one we"
        }
      ]
    },
    {
      "tStartMs": 1133120,
      "dDurationMs": 3520,
      "segs": [
        {
          "utf8": "also know that in 1 one if you pick three and try to move right then you'll"
        }
      ]
    },
    {
      "tStartMs": 1136640,
      "dDurationMs": 2840,
      "segs": [
        {
          "utf8": "stay there because there's a wall in front of it"
        }
      ]
    },
    {
      "tStartMs": 1139480,
      "dDurationMs": 6280,
      "segs": [
        {
          "utf8": "if the agent also understands this then we say that it has a world model or a"
        }
      ]
    },
    {
      "tStartMs": 1145760,
      "dDurationMs": 5159,
      "segs": [
        {
          "utf8": "model of the environment mathematically this just means having access to this"
        }
      ]
    },
    {
      "tStartMs": 1150919,
      "dDurationMs": 4561,
      "segs": [
        {
          "utf8": "function sometimes denoted as P which tells you given that you picked a"
        }
      ]
    },
    {
      "tStartMs": 1155480,
      "dDurationMs": 6360,
      "segs": [
        {
          "utf8": "certain action in a certain State what reward and next state will it lead to or"
        }
      ]
    },
    {
      "tStartMs": 1161840,
      "dDurationMs": 4440,
      "segs": [
        {
          "utf8": "more precisely to account for some Randomness in the environment what's the"
        }
      ]
    },
    {
      "tStartMs": 1166280,
      "dDurationMs": 4480,
      "segs": [
        {
          "utf8": "probability of any next state and reward given a combination of being in a"
        }
      ]
    },
    {
      "tStartMs": 1170760,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "certain State and picking a certain action a function like this is an"
        }
      ]
    },
    {
      "tStartMs": 1175320,
      "dDurationMs": 5760,
      "segs": [
        {
          "utf8": "example of a world model it turns out that whether an RL agent has access to a"
        }
      ]
    },
    {
      "tStartMs": 1181080,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "world model whether it understands how the world Works makes a massive"
        }
      ]
    },
    {
      "tStartMs": 1186120,
      "dDurationMs": 4679,
      "segs": [
        {
          "utf8": "difference to how well it can learn to do tasks which obviously makes sense"
        }
      ]
    },
    {
      "tStartMs": 1190799,
      "dDurationMs": 3880,
      "segs": [
        {
          "utf8": "this is also why a lot of the current research in RL is concerned with how to"
        }
      ]
    },
    {
      "tStartMs": 1194679,
      "dDurationMs": 4201,
      "segs": [
        {
          "utf8": "develop Better World models or make agents learn Better World models on"
        }
      ]
    },
    {
      "tStartMs": 1198880,
      "dDurationMs": 4760,
      "segs": [
        {
          "utf8": "their own for complex tasks with our maze task though we're going to start by"
        }
      ]
    },
    {
      "tStartMs": 1203640,
      "dDurationMs": 4760,
      "segs": [
        {
          "utf8": "focusing on model free methods which assume that the agent does not have"
        }
      ]
    },
    {
      "tStartMs": 1208400,
      "dDurationMs": 4480,
      "segs": [
        {
          "utf8": "access to a world model in other words the agents cannot see the whole maze out"
        }
      ]
    },
    {
      "tStartMs": 1212880,
      "dDurationMs": 4120,
      "segs": [
        {
          "utf8": "a glance like us humans it doesn't know what these State and action numbers"
        }
      ]
    },
    {
      "tStartMs": 1217000,
      "dDurationMs": 3919,
      "segs": [
        {
          "utf8": "represent or how they relate to each other and it doesn't understand how the"
        }
      ]
    },
    {
      "tStartMs": 1220919,
      "dDurationMs": 4401,
      "segs": [
        {
          "utf8": "rewards work so it doesn't know that if you're in 0 0 and you pick three you'll"
        }
      ]
    },
    {
      "tStartMs": 1225320,
      "dDurationMs": 4719,
      "segs": [
        {
          "utf8": "end up in one Z for example these states actions and rewards are just meaningless"
        }
      ]
    },
    {
      "tStartMs": 1230039,
      "dDurationMs": 4161,
      "segs": [
        {
          "utf8": "random numbers and the agent has to somehow learn to pick actions in a way"
        }
      ]
    },
    {
      "tStartMs": 1234200,
      "dDurationMs": 4680,
      "segs": [
        {
          "utf8": "that maximizes the rewards it gets so without access to any information all"
        }
      ]
    },
    {
      "tStartMs": 1238880,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "the agents can do at the start is just randomly pick actions meaning that"
        }
      ]
    },
    {
      "tStartMs": 1243600,
      "dDurationMs": 4559,
      "segs": [
        {
          "utf8": "instead of using a policy where there's actually meaningful probabilities for"
        }
      ]
    },
    {
      "tStartMs": 1248159,
      "dDurationMs": 5640,
      "segs": [
        {
          "utf8": "the different actions it's effectively using a policy that looks like this for"
        }
      ]
    },
    {
      "tStartMs": 1253799,
      "dDurationMs": 4841,
      "segs": [
        {
          "utf8": "all these meaningless input numbers all the meaningless output numbers have the"
        }
      ]
    },
    {
      "tStartMs": 1258640,
      "dDurationMs": 6320,
      "segs": [
        {
          "utf8": "the same probability and we're meant to somehow improve this policy over time so"
        }
      ]
    },
    {
      "tStartMs": 1264960,
      "dDurationMs": 4079,
      "segs": [
        {
          "utf8": "by picking random actions of course most of the time it gets nowhere and receives"
        }
      ]
    },
    {
      "tStartMs": 1269039,
      "dDurationMs": 5801,
      "segs": [
        {
          "utf8": "a total reward of -2 since the episodes are CT at 20 steps but the important"
        }
      ]
    },
    {
      "tStartMs": 1274840,
      "dDurationMs": 5199,
      "segs": [
        {
          "utf8": "thing is that it's writing down all of its experiences in order to learn from"
        }
      ]
    },
    {
      "tStartMs": 1280039,
      "dDurationMs": 4520,
      "segs": [
        {
          "utf8": "this process of trying actions is called sampling and all these sequences of"
        }
      ]
    },
    {
      "tStartMs": 1284559,
      "dDurationMs": 5281,
      "segs": [
        {
          "utf8": "State action reward State action reward are known as trajectories the agent is"
        }
      ]
    },
    {
      "tStartMs": 1289840,
      "dDurationMs": 5120,
      "segs": [
        {
          "utf8": "collecting sample trajectories eventually due to shearlock there will"
        }
      ]
    },
    {
      "tStartMs": 1294960,
      "dDurationMs": 4360,
      "segs": [
        {
          "utf8": "be a few trajectories where it actually does reach the Target and these are the"
        }
      ]
    },
    {
      "tStartMs": 1299320,
      "dDurationMs": 4080,
      "segs": [
        {
          "utf8": "important ones let's see how the Learning Happens so first we're going to"
        }
      ]
    },
    {
      "tStartMs": 1303400,
      "dDurationMs": 4200,
      "segs": [
        {
          "utf8": "now use this other mathematical Concepts that we've developed and calculate the"
        }
      ]
    },
    {
      "tStartMs": 1307600,
      "dDurationMs": 5559,
      "segs": [
        {
          "utf8": "returns for each time step in our successful trajectory using a gamma of"
        }
      ]
    },
    {
      "tStartMs": 1313159,
      "dDurationMs": 4041,
      "segs": [
        {
          "utf8": "let's say 0.8 the way we can do this because of"
        }
      ]
    },
    {
      "tStartMs": 1317200,
      "dDurationMs": 5079,
      "segs": [
        {
          "utf8": "the way that later boards are progressively multiplied by gamma is we"
        }
      ]
    },
    {
      "tStartMs": 1322279,
      "dDurationMs": 5441,
      "segs": [
        {
          "utf8": "can start at the end and work backwards the return of the last time step will"
        }
      ]
    },
    {
      "tStartMs": 1327720,
      "dDurationMs": 2640,
      "segs": [
        {
          "utf8": "just be"
        }
      ]
    },
    {
      "tStartMs": 1330440,
      "dDurationMs": 5719,
      "segs": [
        {
          "utf8": "zero and to get to the previous one you multiply that by gamma and then add on"
        }
      ]
    },
    {
      "tStartMs": 1336159,
      "dDurationMs": 3041,
      "segs": [
        {
          "utf8": "the previous"
        }
      ]
    },
    {
      "tStartMs": 1339720,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "reward then to get to the previous one you multiply that whole sum by gamma and"
        }
      ]
    },
    {
      "tStartMs": 1345000,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "add on the previous reward again"
        }
      ]
    },
    {
      "tStartMs": 1352440,
      "dDurationMs": 4320,
      "segs": [
        {
          "utf8": "then you repeat in the end you'll find that even though this is a successful"
        }
      ]
    },
    {
      "tStartMs": 1356760,
      "dDurationMs": 4880,
      "segs": [
        {
          "utf8": "trajectory all the returns are negative because all the rewards are negative"
        }
      ]
    },
    {
      "tStartMs": 1361640,
      "dDurationMs": 3720,
      "segs": [
        {
          "utf8": "obviously except for when the agent reaches the target but in the"
        }
      ]
    },
    {
      "tStartMs": 1365360,
      "dDurationMs": 4040,
      "segs": [
        {
          "utf8": "unsuccessful trajectories all the calculated returns are going to be even"
        }
      ]
    },
    {
      "tStartMs": 1369400,
      "dDurationMs": 4840,
      "segs": [
        {
          "utf8": "more negative because the agent never reaches the target so keeps collecting"
        }
      ]
    },
    {
      "tStartMs": 1374240,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "the minus one rewards for the full 20 time steps so now that we have all our"
        }
      ]
    },
    {
      "tStartMs": 1379200,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "sample trajectories and we've calculated the returns for each time step we have"
        }
      ]
    },
    {
      "tStartMs": 1383360,
      "dDurationMs": 4600,
      "segs": [
        {
          "utf8": "to somehow use these experiences to achieve a better policy there are"
        }
      ]
    },
    {
      "tStartMs": 1387960,
      "dDurationMs": 3440,
      "segs": [
        {
          "utf8": "different ways that you can go about this you can directly work with these"
        }
      ]
    },
    {
      "tStartMs": 1391400,
      "dDurationMs": 4600,
      "segs": [
        {
          "utf8": "probabilities for example push the probability up for actions that had good"
        }
      ]
    },
    {
      "tStartMs": 1396000,
      "dDurationMs": 4919,
      "segs": [
        {
          "utf8": "returns and push the probability down for actions that had bad returns that"
        }
      ]
    },
    {
      "tStartMs": 1400919,
      "dDurationMs": 5601,
      "segs": [
        {
          "utf8": "would be called a policy gradients method gradient meaning rate of change"
        }
      ]
    },
    {
      "tStartMs": 1406520,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "and we have to calculate how much to change the policy based on which actions"
        }
      ]
    },
    {
      "tStartMs": 1411080,
      "dDurationMs": 6199,
      "segs": [
        {
          "utf8": "are good and which actions are bad but doing that requires us to know what's"
        }
      ]
    },
    {
      "tStartMs": 1417279,
      "dDurationMs": 4961,
      "segs": [
        {
          "utf8": "considered good and what's considered bad consider this example where we have"
        }
      ]
    },
    {
      "tStartMs": 1422240,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "three actions initially all set to the same probability let's say we collect a"
        }
      ]
    },
    {
      "tStartMs": 1426960,
      "dDurationMs": 6360,
      "segs": [
        {
          "utf8": "lot of sample trajectories and on average action one gives a return of -5"
        }
      ]
    },
    {
      "tStartMs": 1433320,
      "dDurationMs": 6239,
      "segs": [
        {
          "utf8": "action two gives a return of zero and the third action gives a return of five"
        }
      ]
    },
    {
      "tStartMs": 1439559,
      "dDurationMs": 4480,
      "segs": [
        {
          "utf8": "then it's pretty straightforward you push the probability for action one down"
        }
      ]
    },
    {
      "tStartMs": 1444039,
      "dDurationMs": 5041,
      "segs": [
        {
          "utf8": "keep action two roughly the same and then push action three up but what if"
        }
      ]
    },
    {
      "tStartMs": 1449080,
      "dDurationMs": 7839,
      "segs": [
        {
          "utf8": "instead action one gave five action two gave 10 and action 3 gave 15 or what if"
        }
      ]
    },
    {
      "tStartMs": 1456919,
      "dDurationMs": 6841,
      "segs": [
        {
          "utf8": "it's 95 100 and 105 or if it's like our maze example where everything's negative"
        }
      ]
    },
    {
      "tStartMs": 1463760,
      "dDurationMs": 4360,
      "segs": [
        {
          "utf8": "see the issue here is not that all three numbers are positive or negative and you"
        }
      ]
    },
    {
      "tStartMs": 1468120,
      "dDurationMs": 4120,
      "segs": [
        {
          "utf8": "can increase or decrease all probabilities and still have them add up"
        }
      ]
    },
    {
      "tStartMs": 1472240,
      "dDurationMs": 4880,
      "segs": [
        {
          "utf8": "to one when you're actually using policy gradient methods the effect on the"
        }
      ]
    },
    {
      "tStartMs": 1477120,
      "dDurationMs": 5279,
      "segs": [
        {
          "utf8": "probabilities is more like a tug of war especially with neural networks which"
        }
      ]
    },
    {
      "tStartMs": 1482399,
      "dDurationMs": 3640,
      "segs": [
        {
          "utf8": "have this thing called a soft Max function that makes sure everything adds"
        }
      ]
    },
    {
      "tStartMs": 1486039,
      "dDurationMs": 4441,
      "segs": [
        {
          "utf8": "up to one the real issue is that proportionally the effects are now"
        }
      ]
    },
    {
      "tStartMs": 1490480,
      "dDurationMs": 7880,
      "segs": [
        {
          "utf8": "different 10 to 15 is a 50% increase while 100 to 105 is only a 5% increase"
        }
      ]
    },
    {
      "tStartMs": 1498360,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "increase what we want is for all cases to have the same effect because an"
        }
      ]
    },
    {
      "tStartMs": 1503000,
      "dDurationMs": 4120,
      "segs": [
        {
          "utf8": "increase of five is an increase of five no matter what especially since within"
        }
      ]
    },
    {
      "tStartMs": 1507120,
      "dDurationMs": 4480,
      "segs": [
        {
          "utf8": "the same environment maybe some states are just better on average and some"
        }
      ]
    },
    {
      "tStartMs": 1511600,
      "dDurationMs": 4040,
      "segs": [
        {
          "utf8": "states are just worse on average no matter what action you take so what we"
        }
      ]
    },
    {
      "tStartMs": 1515640,
      "dDurationMs": 5480,
      "segs": [
        {
          "utf8": "need is these things called value functions as our baselines which keep"
        }
      ]
    },
    {
      "tStartMs": 1521120,
      "dDurationMs": 3400,
      "segs": [
        {
          "utf8": "track of the average return you should expect to get when you're following a"
        }
      ]
    },
    {
      "tStartMs": 1524520,
      "dDurationMs": 6879,
      "segs": [
        {
          "utf8": "certain policy and help evaluate how good or bad States and actions are V is"
        }
      ]
    },
    {
      "tStartMs": 1531399,
      "dDurationMs": 4361,
      "segs": [
        {
          "utf8": "the state value function representing how much return you should expect to get"
        }
      ]
    },
    {
      "tStartMs": 1535760,
      "dDurationMs": 6680,
      "segs": [
        {
          "utf8": "when you're in a certain State and follow a certain policy Q is the action"
        }
      ]
    },
    {
      "tStartMs": 1542440,
      "dDurationMs": 4080,
      "segs": [
        {
          "utf8": "value function representing how much return you should expect if you're in a"
        }
      ]
    },
    {
      "tStartMs": 1546520,
      "dDurationMs": 6639,
      "segs": [
        {
          "utf8": "certain State pick a certain action and then follow the given policy notice that"
        }
      ]
    },
    {
      "tStartMs": 1553159,
      "dDurationMs": 4721,
      "segs": [
        {
          "utf8": "these functions necessarily have a policy function attached to them because"
        }
      ]
    },
    {
      "tStartMs": 1557880,
      "dDurationMs": 4880,
      "segs": [
        {
          "utf8": "if it it's a different policy function the actions that are picked will be"
        }
      ]
    },
    {
      "tStartMs": 1562760,
      "dDurationMs": 5399,
      "segs": [
        {
          "utf8": "different so naturally you would expect to get different returns these are just"
        }
      ]
    },
    {
      "tStartMs": 1568159,
      "dDurationMs": 3801,
      "segs": [
        {
          "utf8": "random example numbers that I made up but the key is that it's different for"
        }
      ]
    },
    {
      "tStartMs": 1571960,
      "dDurationMs": 3560,
      "segs": [
        {
          "utf8": "different policies in fact the state value"
        }
      ]
    },
    {
      "tStartMs": 1575520,
      "dDurationMs": 5680,
      "segs": [
        {
          "utf8": "function is basically just all the action values within that state averaged"
        }
      ]
    },
    {
      "tStartMs": 1581200,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "according to the probabilities given by the policy if you just want to not care"
        }
      ]
    },
    {
      "tStartMs": 1586480,
      "dDurationMs": 4360,
      "segs": [
        {
          "utf8": "about which specific policy is talking about and refer to what's the best case"
        }
      ]
    },
    {
      "tStartMs": 1590840,
      "dDurationMs": 5680,
      "segs": [
        {
          "utf8": "scenario if you somehow picked the best action every time then that would be V"
        }
      ]
    },
    {
      "tStartMs": 1596520,
      "dDurationMs": 6200,
      "segs": [
        {
          "utf8": "and qar the optimal value functions we have written these value functions not"
        }
      ]
    },
    {
      "tStartMs": 1602720,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "as probabilities without any Randomness unlike the policy and World model up"
        }
      ]
    },
    {
      "tStartMs": 1608720,
      "dDurationMs": 5559,
      "segs": [
        {
          "utf8": "here because RL is still developing and up till now people have not yet"
        }
      ]
    },
    {
      "tStartMs": 1614279,
      "dDurationMs": 5321,
      "segs": [
        {
          "utf8": "considered Randomness and variance for the value functions as much there are"
        }
      ]
    },
    {
      "tStartMs": 1619600,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "fields of research though like Bayesian RL and distributional RL which do"
        }
      ]
    },
    {
      "tStartMs": 1624960,
      "dDurationMs": 4360,
      "segs": [
        {
          "utf8": "consider variants in value functions which you can think of as like Risk to"
        }
      ]
    },
    {
      "tStartMs": 1629320,
      "dDurationMs": 6479,
      "segs": [
        {
          "utf8": "reward ratio as in how good a state or action is but also how uncertain that"
        }
      ]
    },
    {
      "tStartMs": 1635799,
      "dDurationMs": 6521,
      "segs": [
        {
          "utf8": "goodness is there are so many unanswered questions in RL but anyways that's our"
        }
      ]
    },
    {
      "tStartMs": 1642320,
      "dDurationMs": 6359,
      "segs": [
        {
          "utf8": "value functions once we have our value functions then we can improve our policy"
        }
      ]
    },
    {
      "tStartMs": 1648679,
      "dDurationMs": 5041,
      "segs": [
        {
          "utf8": "and now we have a few different options as to how we do this first we could just"
        }
      ]
    },
    {
      "tStartMs": 1653720,
      "dDurationMs": 4520,
      "segs": [
        {
          "utf8": "use the policy gradient method we talked about before without using a value"
        }
      ]
    },
    {
      "tStartMs": 1658240,
      "dDurationMs": 4600,
      "segs": [
        {
          "utf8": "function Baseline there's issues with that as we've said with the different"
        }
      ]
    },
    {
      "tStartMs": 1662840,
      "dDurationMs": 4400,
      "segs": [
        {
          "utf8": "averages it doesn't work that well and then we could also use the policy"
        }
      ]
    },
    {
      "tStartMs": 1667240,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "gradient method with a baseline such as our state value function to compare with"
        }
      ]
    },
    {
      "tStartMs": 1672600,
      "dDurationMs": 4600,
      "segs": [
        {
          "utf8": "all the different returns in the same state now this works better as all these"
        }
      ]
    },
    {
      "tStartMs": 1677200,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "cases with different averages can be made to all have the same effect but it"
        }
      ]
    },
    {
      "tStartMs": 1681760,
      "dDurationMs": 5320,
      "segs": [
        {
          "utf8": "turns out there's also another method which is instead to just look at the"
        }
      ]
    },
    {
      "tStartMs": 1687080,
      "dDurationMs": 6479,
      "segs": [
        {
          "utf8": "action value function and pick the action with the highest estimate every"
        }
      ]
    },
    {
      "tStartMs": 1693559,
      "dDurationMs": 5240,
      "segs": [
        {
          "utf8": "time in other words the policy doesn't keep track of different probabilities"
        }
      ]
    },
    {
      "tStartMs": 1698799,
      "dDurationMs": 4921,
      "segs": [
        {
          "utf8": "anymore 100% of the probability just goes to the action with the highest"
        }
      ]
    },
    {
      "tStartMs": 1703720,
      "dDurationMs": 3839,
      "segs": [
        {
          "utf8": "estimate well not quite because some Randomness is needed or else there will"
        }
      ]
    },
    {
      "tStartMs": 1707559,
      "dDurationMs": 5281,
      "segs": [
        {
          "utf8": "be issues as we'll see but now there's this cyclic dependency because usually"
        }
      ]
    },
    {
      "tStartMs": 1712840,
      "dDurationMs": 5199,
      "segs": [
        {
          "utf8": "the action value function is based on the policy function but now the policy"
        }
      ]
    },
    {
      "tStartMs": 1718039,
      "dDurationMs": 5201,
      "segs": [
        {
          "utf8": "function is also based on the action value function so with this cyclic"
        }
      ]
    },
    {
      "tStartMs": 1723240,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "nature as long as you keep improving the accuracy of the action value function as"
        }
      ]
    },
    {
      "tStartMs": 1728200,
      "dDurationMs": 6240,
      "segs": [
        {
          "utf8": "we'll see later the policy will improve alongside it out of the three methods"
        }
      ]
    },
    {
      "tStartMs": 1734440,
      "dDurationMs": 4239,
      "segs": [
        {
          "utf8": "you can essentially think of the first method as being more depend dep on the"
        }
      ]
    },
    {
      "tStartMs": 1738679,
      "dDurationMs": 6081,
      "segs": [
        {
          "utf8": "policy function the last method as being more dependent on a value function and"
        }
      ]
    },
    {
      "tStartMs": 1744760,
      "dDurationMs": 4200,
      "segs": [
        {
          "utf8": "the middle method as being a balance between the two taking the Best of Both"
        }
      ]
    },
    {
      "tStartMs": 1748960,
      "dDurationMs": 4000,
      "segs": [
        {
          "utf8": "Worlds between the policy and the value function all of them have their"
        }
      ]
    },
    {
      "tStartMs": 1752960,
      "dDurationMs": 4520,
      "segs": [
        {
          "utf8": "advantages and disadvantages but this balanced method does tend to be the most"
        }
      ]
    },
    {
      "tStartMs": 1757480,
      "dDurationMs": 5120,
      "segs": [
        {
          "utf8": "powerful out of the three what I want to do though is focus exclusively on the"
        }
      ]
    },
    {
      "tStartMs": 1762600,
      "dDurationMs": 5679,
      "segs": [
        {
          "utf8": "problem of evaluation for now the problem of how to develop good value"
        }
      ]
    },
    {
      "tStartMs": 1768279,
      "dDurationMs": 5520,
      "segs": [
        {
          "utf8": "functions having good evaluation is very important to having good Improvement so"
        }
      ]
    },
    {
      "tStartMs": 1773799,
      "dDurationMs": 5480,
      "segs": [
        {
          "utf8": "let's first focus on this last method this action Value method even though it"
        }
      ]
    },
    {
      "tStartMs": 1779279,
      "dDurationMs": 4801,
      "segs": [
        {
          "utf8": "has its disadvantages which we'll get to later it allows us to not worry about"
        }
      ]
    },
    {
      "tStartMs": 1784080,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "the policy function for now and just focus on some key concepts for good"
        }
      ]
    },
    {
      "tStartMs": 1789120,
      "dDurationMs": 4600,
      "segs": [
        {
          "utf8": "evaluation so of course the whole point of evaluation is we don't just magically"
        }
      ]
    },
    {
      "tStartMs": 1793720,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "have access to such an action value Q function and must devel velop one and"
        }
      ]
    },
    {
      "tStartMs": 1798760,
      "dDurationMs": 5039,
      "segs": [
        {
          "utf8": "make it more accurate over time the simplest way we can do this is just to"
        }
      ]
    },
    {
      "tStartMs": 1803799,
      "dDurationMs": 4681,
      "segs": [
        {
          "utf8": "take the trajectories we collected and then average out the return values for"
        }
      ]
    },
    {
      "tStartMs": 1808480,
      "dDurationMs": 5160,
      "segs": [
        {
          "utf8": "each combination of state and action we can imagine that for every possible"
        }
      ]
    },
    {
      "tStartMs": 1813640,
      "dDurationMs": 5560,
      "segs": [
        {
          "utf8": "State action input the output of this Q function will initially be set to zero"
        }
      ]
    },
    {
      "tStartMs": 1819200,
      "dDurationMs": 4520,
      "segs": [
        {
          "utf8": "as shown in this table of course the agents has no knowledge that the table"
        }
      ]
    },
    {
      "tStartMs": 1823720,
      "dDurationMs": 5439,
      "segs": [
        {
          "utf8": "is supposed to be a 5x3 grid like the maze but I set it up like this just to"
        }
      ]
    },
    {
      "tStartMs": 1829159,
      "dDurationMs": 4760,
      "segs": [
        {
          "utf8": "show the values that will actually end up changing so with every new return"
        }
      ]
    },
    {
      "tStartMs": 1833919,
      "dDurationMs": 5281,
      "segs": [
        {
          "utf8": "value remember we're working backwards so let's say this state of 41 plus this"
        }
      ]
    },
    {
      "tStartMs": 1839200,
      "dDurationMs": 5719,
      "segs": [
        {
          "utf8": "action of one which would correspond to this right here we would move the value"
        }
      ]
    },
    {
      "tStartMs": 1844919,
      "dDurationMs": 5681,
      "segs": [
        {
          "utf8": "for that specific State action pair towards this result by a small amount"
        }
      ]
    },
    {
      "tStartMs": 1850600,
      "dDurationMs": 4319,
      "segs": [
        {
          "utf8": "let's say 10% now in this case since they're both zero it wouldn't matter but"
        }
      ]
    },
    {
      "tStartMs": 1854919,
      "dDurationMs": 8480,
      "segs": [
        {
          "utf8": "it would be with a calculation like this 0 Plus + 0 - 0 * 10% which is 0.1 this"
        }
      ]
    },
    {
      "tStartMs": 1863399,
      "dDurationMs": 4760,
      "segs": [
        {
          "utf8": "is an example of how we could gradually adjust these values of course it'll be"
        }
      ]
    },
    {
      "tStartMs": 1868159,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "zero so nothing will change but let's say the previous one with 3 1 and 3 the"
        }
      ]
    },
    {
      "tStartMs": 1873519,
      "dDurationMs": 8481,
      "segs": [
        {
          "utf8": "result is -1 so we would say 0 + -1 - 0 * 0.1 and then we would update our table"
        }
      ]
    },
    {
      "tStartMs": 1882000,
      "dDurationMs": 3760,
      "segs": [
        {
          "utf8": "with this new result now for the sake of time I'm not going to do all of the"
        }
      ]
    },
    {
      "tStartMs": 1885760,
      "dDurationMs": 4080,
      "segs": [
        {
          "utf8": "values but just to demonstrate here we have have two instances of the same"
        }
      ]
    },
    {
      "tStartMs": 1889840,
      "dDurationMs": 6520,
      "segs": [
        {
          "utf8": "action value pair of 103 so for example first of all it would be - 4.57 so that"
        }
      ]
    },
    {
      "tStartMs": 1896360,
      "dDurationMs": 5000,
      "segs": [
        {
          "utf8": "our result is now - 0.457 and then when we get to the other"
        }
      ]
    },
    {
      "tStartMs": 1901360,
      "dDurationMs": 5120,
      "segs": [
        {
          "utf8": "one now we have - 0.457 cuz it's what value was originally"
        }
      ]
    },
    {
      "tStartMs": 1906480,
      "dDurationMs": 6319,
      "segs": [
        {
          "utf8": "there plus the new result of - 4.73 minus the existing value and then time"
        }
      ]
    },
    {
      "tStartMs": 1912799,
      "dDurationMs": 6561,
      "segs": [
        {
          "utf8": "0.1 so the new value is now more negative and adjusted towards whatever"
        }
      ]
    },
    {
      "tStartMs": 1919360,
      "dDurationMs": 5319,
      "segs": [
        {
          "utf8": "results we are using so this constant shifting of our values towards these new"
        }
      ]
    },
    {
      "tStartMs": 1924679,
      "dDurationMs": 4761,
      "segs": [
        {
          "utf8": "results by a small amount is how everything gets averaged over time and"
        }
      ]
    },
    {
      "tStartMs": 1929440,
      "dDurationMs": 4920,
      "segs": [
        {
          "utf8": "over many iterations the good States and actions will end up having a good value"
        }
      ]
    },
    {
      "tStartMs": 1934360,
      "dDurationMs": 4600,
      "segs": [
        {
          "utf8": "in this function and the bad States and actions will have bad values ideally"
        }
      ]
    },
    {
      "tStartMs": 1938960,
      "dDurationMs": 4559,
      "segs": [
        {
          "utf8": "eventually it'll be able to approach our actual environment obviously the states"
        }
      ]
    },
    {
      "tStartMs": 1943519,
      "dDurationMs": 3520,
      "segs": [
        {
          "utf8": "that are closer to our Target should have better values and the states that"
        }
      ]
    },
    {
      "tStartMs": 1947039,
      "dDurationMs": 4760,
      "segs": [
        {
          "utf8": "are farther away should have worse so let's run the simulation for lots and"
        }
      ]
    },
    {
      "tStartMs": 1951799,
      "dDurationMs": 5201,
      "segs": [
        {
          "utf8": "lots of episodes in the beginning the trajectories don't really get anywhere"
        }
      ]
    },
    {
      "tStartMs": 1957000,
      "dDurationMs": 4679,
      "segs": [
        {
          "utf8": "but what you see is over time the Q function that we are developing which"
        }
      ]
    },
    {
      "tStartMs": 1961679,
      "dDurationMs": 4201,
      "segs": [
        {
          "utf8": "evaluates the policy becomes more accurate and because it becomes more"
        }
      ]
    },
    {
      "tStartMs": 1965880,
      "dDurationMs": 5120,
      "segs": [
        {
          "utf8": "accurate the policy which picks the best action based on the Q function also"
        }
      ]
    },
    {
      "tStartMs": 1971000,
      "dDurationMs": 4600,
      "segs": [
        {
          "utf8": "improves but because the policy improved the Q function is now evaluating a"
        }
      ]
    },
    {
      "tStartMs": 1975600,
      "dDurationMs": 4439,
      "segs": [
        {
          "utf8": "slightly different policy and has has to become more accurate for the new policy"
        }
      ]
    },
    {
      "tStartMs": 1980039,
      "dDurationMs": 4961,
      "segs": [
        {
          "utf8": "yet again and then after that the policy improves again because of this cyclic"
        }
      ]
    },
    {
      "tStartMs": 1985000,
      "dDurationMs": 5159,
      "segs": [
        {
          "utf8": "dependence between the policy and the Q function the Improvement happens in a"
        }
      ]
    },
    {
      "tStartMs": 1990159,
      "dDurationMs": 6161,
      "segs": [
        {
          "utf8": "cycle known as generalized policy iteration eventually the goal is for the"
        }
      ]
    },
    {
      "tStartMs": 1996320,
      "dDurationMs": 5120,
      "segs": [
        {
          "utf8": "Q function to approach the Q star function where it accounts for the best"
        }
      ]
    },
    {
      "tStartMs": 2001440,
      "dDurationMs": 4599,
      "segs": [
        {
          "utf8": "case scenario out of all the different ways of picking actions then naturally"
        }
      ]
    },
    {
      "tStartMs": 2006039,
      "dDurationMs": 5760,
      "segs": [
        {
          "utf8": "the policy based on this op optimal qar function will also be the optimal policy"
        }
      ]
    },
    {
      "tStartMs": 2011799,
      "dDurationMs": 4681,
      "segs": [
        {
          "utf8": "here in the corner we have some results to track how the agent is improving but"
        }
      ]
    },
    {
      "tStartMs": 2016480,
      "dDurationMs": 5720,
      "segs": [
        {
          "utf8": "often times in RL you track Improvement using total undiscounted reward as"
        }
      ]
    },
    {
      "tStartMs": 2022200,
      "dDurationMs": 5599,
      "segs": [
        {
          "utf8": "opposed to discounted return which is what's used for the training that's not"
        }
      ]
    },
    {
      "tStartMs": 2027799,
      "dDurationMs": 6321,
      "segs": [
        {
          "utf8": "too important but what is more important is this variable over here called"
        }
      ]
    },
    {
      "tStartMs": 2034120,
      "dDurationMs": 4399,
      "segs": [
        {
          "utf8": "Epsilon and what it does is help maintain a balance between"
        }
      ]
    },
    {
      "tStartMs": 2038519,
      "dDurationMs": 5921,
      "segs": [
        {
          "utf8": "exploration and exploitation this is one of the downsides of this action Value"
        }
      ]
    },
    {
      "tStartMs": 2044440,
      "dDurationMs": 4719,
      "segs": [
        {
          "utf8": "method the reason the agent was able to develop the value function in the first"
        }
      ]
    },
    {
      "tStartMs": 2049159,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "place was by following a random policy and gaining all these diverse"
        }
      ]
    },
    {
      "tStartMs": 2054599,
      "dDurationMs": 4841,
      "segs": [
        {
          "utf8": "experiences that's known as exploration but once it actually wants"
        }
      ]
    },
    {
      "tStartMs": 2059440,
      "dDurationMs": 4719,
      "segs": [
        {
          "utf8": "to use the value function to perform better and get more rewards known as"
        }
      ]
    },
    {
      "tStartMs": 2064159,
      "dDurationMs": 5321,
      "segs": [
        {
          "utf8": "exploitation then if it just picks the action with the highest value every time"
        }
      ]
    },
    {
      "tStartMs": 2069480,
      "dDurationMs": 4439,
      "segs": [
        {
          "utf8": "then there's no Randomness anymore the probability distribution ends up being"
        }
      ]
    },
    {
      "tStartMs": 2073919,
      "dDurationMs": 5321,
      "segs": [
        {
          "utf8": "one action having 100% probability and all the other actions having zero when"
        }
      ]
    },
    {
      "tStartMs": 2079240,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "you're improving in RL what you need is a balance between exploration and"
        }
      ]
    },
    {
      "tStartMs": 2083960,
      "dDurationMs": 5000,
      "segs": [
        {
          "utf8": "exploitation so that you can both try out different things to improve and also"
        }
      ]
    },
    {
      "tStartMs": 2088960,
      "dDurationMs": 4959,
      "segs": [
        {
          "utf8": "take advantage of that Improvement to perform well now with policy gradients"
        }
      ]
    },
    {
      "tStartMs": 2093919,
      "dDurationMs": 4200,
      "segs": [
        {
          "utf8": "methods that is not a concern since you're adjusting a whole bunch of"
        }
      ]
    },
    {
      "tStartMs": 2098119,
      "dDurationMs": 4521,
      "segs": [
        {
          "utf8": "different probabilities it'll naturally have some Randomness even if certain"
        }
      ]
    },
    {
      "tStartMs": 2102640,
      "dDurationMs": 4360,
      "segs": [
        {
          "utf8": "actions tend to be more optimal but with this action Value method where you're"
        }
      ]
    },
    {
      "tStartMs": 2107000,
      "dDurationMs": 6240,
      "segs": [
        {
          "utf8": "just picking the best one we have to use this hack called Epsilon greedy to kind"
        }
      ]
    },
    {
      "tStartMs": 2113240,
      "dDurationMs": 5240,
      "segs": [
        {
          "utf8": "of artificially introduce some Randomness back in basically Epsilon"
        }
      ]
    },
    {
      "tStartMs": 2118480,
      "dDurationMs": 4280,
      "segs": [
        {
          "utf8": "represents the proportion of the time where the agent picks a random action"
        }
      ]
    },
    {
      "tStartMs": 2122760,
      "dDurationMs": 5240,
      "segs": [
        {
          "utf8": "and it's supposed to gradually decrease for example you might have Epsilon start"
        }
      ]
    },
    {
      "tStartMs": 2128000,
      "dDurationMs": 6480,
      "segs": [
        {
          "utf8": "off as 0.9 meaning you pick randomly 90% of the time and pick optimally 10% of"
        }
      ]
    },
    {
      "tStartMs": 2134480,
      "dDurationMs": 4440,
      "segs": [
        {
          "utf8": "the time this is because what the agent thinks is optimal is not very accurate"
        }
      ]
    },
    {
      "tStartMs": 2138920,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "at the start but then as time goes on and the action value estimates become"
        }
      ]
    },
    {
      "tStartMs": 2143880,
      "dDurationMs": 4239,
      "segs": [
        {
          "utf8": "more accurate Epsilon gradually decreases to"
        }
      ]
    },
    {
      "tStartMs": 2148119,
      "dDurationMs": 6960,
      "segs": [
        {
          "utf8": "0.1 which is 10% random 90% optimal this method that we just covered is known as"
        }
      ]
    },
    {
      "tStartMs": 2155079,
      "dDurationMs": 6881,
      "segs": [
        {
          "utf8": "a Monty Carlo method which according to Wikipedia is just a computational"
        }
      ]
    },
    {
      "tStartMs": 2161960,
      "dDurationMs": 5760,
      "segs": [
        {
          "utf8": "algorithm that relies on repeated random sampling to obtain numerical results we"
        }
      ]
    },
    {
      "tStartMs": 2167720,
      "dDurationMs": 5480,
      "segs": [
        {
          "utf8": "used random sample trajectories to figure out what the values of the Q"
        }
      ]
    },
    {
      "tStartMs": 2173200,
      "dDurationMs": 4480,
      "segs": [
        {
          "utf8": "function should be now let's look at what the limitations of this method are"
        }
      ]
    },
    {
      "tStartMs": 2177680,
      "dDurationMs": 4520,
      "segs": [
        {
          "utf8": "and what we can improve upon it"
        }
      ]
    },
    {
      "tStartMs": 2183830,
      "dDurationMs": 3090,
      "segs": [
        {
          "utf8": "[Music]"
        }
      ]
    },
    {
      "tStartMs": 2188400,
      "dDurationMs": 4679,
      "segs": [
        {
          "utf8": "the main issue with Monte Carlo methods like this is that it works on an episode"
        }
      ]
    },
    {
      "tStartMs": 2193079,
      "dDurationMs": 5641,
      "segs": [
        {
          "utf8": "to episode basis when we collect our trajectories we've got good episodes and"
        }
      ]
    },
    {
      "tStartMs": 2198720,
      "dDurationMs": 6639,
      "segs": [
        {
          "utf8": "Bad episodes in the good episodes all the actions will be evaluated better and"
        }
      ]
    },
    {
      "tStartMs": 2205359,
      "dDurationMs": 6201,
      "segs": [
        {
          "utf8": "in the Bad episodes all the actions will be evaluated worse the first problem"
        }
      ]
    },
    {
      "tStartMs": 2211560,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "this results in is that you have to wait for the episode to end before you can"
        }
      ]
    },
    {
      "tStartMs": 2215720,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "even calculate any returns and adjust your evaluations for the actions if the"
        }
      ]
    },
    {
      "tStartMs": 2220440,
      "dDurationMs": 4360,
      "segs": [
        {
          "utf8": "episodes are really long then that's going to be really slow and if it's a"
        }
      ]
    },
    {
      "tStartMs": 2224800,
      "dDurationMs": 4559,
      "segs": [
        {
          "utf8": "continuing task meaning it just keeps on going and doesn't terminate then this"
        }
      ]
    },
    {
      "tStartMs": 2229359,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "method won't even work the second problem is that within the same episode"
        }
      ]
    },
    {
      "tStartMs": 2234319,
      "dDurationMs": 4721,
      "segs": [
        {
          "utf8": "you don't know which actions were good and which actions were bad take our"
        }
      ]
    },
    {
      "tStartMs": 2239040,
      "dDurationMs": 4400,
      "segs": [
        {
          "utf8": "successful trajectory for example if we draw out what the path actually looks"
        }
      ]
    },
    {
      "tStartMs": 2243440,
      "dDurationMs": 4080,
      "segs": [
        {
          "utf8": "like on the maze then it's clear that some actions were the right decision and"
        }
      ]
    },
    {
      "tStartMs": 2247520,
      "dDurationMs": 4920,
      "segs": [
        {
          "utf8": "and some were not using Monte Carlo methods we can only rely on the large"
        }
      ]
    },
    {
      "tStartMs": 2252440,
      "dDurationMs": 6360,
      "segs": [
        {
          "utf8": "number of samples to average out and hopefully correct for this suboptimal"
        }
      ]
    },
    {
      "tStartMs": 2258800,
      "dDurationMs": 5480,
      "segs": [
        {
          "utf8": "action we have now run into the credit assignment problem the problem of"
        }
      ]
    },
    {
      "tStartMs": 2264280,
      "dDurationMs": 4120,
      "segs": [
        {
          "utf8": "figuring out what the impact of an individual action was within a long"
        }
      ]
    },
    {
      "tStartMs": 2268400,
      "dDurationMs": 4360,
      "segs": [
        {
          "utf8": "sequence of many actions so how can we try to solve the credit assignment"
        }
      ]
    },
    {
      "tStartMs": 2272760,
      "dDurationMs": 4040,
      "segs": [
        {
          "utf8": "problem if the agent is just working with these trajectories and cannot see"
        }
      ]
    },
    {
      "tStartMs": 2276800,
      "dDurationMs": 6039,
      "segs": [
        {
          "utf8": "the whole Ma a glance like us humans the answer is a concept known as temporal"
        }
      ]
    },
    {
      "tStartMs": 2282839,
      "dDurationMs": 5401,
      "segs": [
        {
          "utf8": "difference which breaks down evaluation to an action by action basis rather than"
        }
      ]
    },
    {
      "tStartMs": 2288240,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "an episode by episode basis here let's imagine that in these two graphs the"
        }
      ]
    },
    {
      "tStartMs": 2293520,
      "dDurationMs": 4400,
      "segs": [
        {
          "utf8": "magnets represent time steps in a trajectory and we're using them to help"
        }
      ]
    },
    {
      "tStartMs": 2297920,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "construct a q function which estimates how much return you'll get if you're in"
        }
      ]
    },
    {
      "tStartMs": 2302960,
      "dDurationMs": 7200,
      "segs": [
        {
          "utf8": "a state s take action a and then follow policy high in Monte Carlo methods"
        }
      ]
    },
    {
      "tStartMs": 2310160,
      "dDurationMs": 5159,
      "segs": [
        {
          "utf8": "you're directly calculating the returns in an absolute sense by adding up all"
        }
      ]
    },
    {
      "tStartMs": 2315319,
      "dDurationMs": 3921,
      "segs": [
        {
          "utf8": "these subsequent rewards like we've done on paper with these trajectories in"
        }
      ]
    },
    {
      "tStartMs": 2319240,
      "dDurationMs": 4040,
      "segs": [
        {
          "utf8": "order to contribute to this estimate that's why you have to wait until the"
        }
      ]
    },
    {
      "tStartMs": 2323280,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "episode ends however in a temporal difference method even though we're"
        }
      ]
    },
    {
      "tStartMs": 2328240,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "still trying to construct an estimate of the returns we don't directly calculate"
        }
      ]
    },
    {
      "tStartMs": 2333200,
      "dDurationMs": 4440,
      "segs": [
        {
          "utf8": "the returns by adding up all these subsequent rewards instead you come up"
        }
      ]
    },
    {
      "tStartMs": 2337640,
      "dDurationMs": 6320,
      "segs": [
        {
          "utf8": "with an estimate for each state action pair relative to the estimate for the"
        }
      ]
    },
    {
      "tStartMs": 2343960,
      "dDurationMs": 6119,
      "segs": [
        {
          "utf8": "state action pair in the time step after it using just one reward as the"
        }
      ]
    },
    {
      "tStartMs": 2350079,
      "dDurationMs": 5121,
      "segs": [
        {
          "utf8": "difference not all the subsequent rewards the terminal States at the end"
        }
      ]
    },
    {
      "tStartMs": 2355200,
      "dDurationMs": 6360,
      "segs": [
        {
          "utf8": "of episodes which don't have a reward and next state afterwards stay at zero"
        }
      ]
    },
    {
      "tStartMs": 2361560,
      "dDurationMs": 5200,
      "segs": [
        {
          "utf8": "acting as an anchor around which the other values develop if it's a task that"
        }
      ]
    },
    {
      "tStartMs": 2366760,
      "dDurationMs": 5079,
      "segs": [
        {
          "utf8": "continues for forever and there is no terminal State then there is no absolute"
        }
      ]
    },
    {
      "tStartMs": 2371839,
      "dDurationMs": 4921,
      "segs": [
        {
          "utf8": "reference point as an anchor and all the values might drift up or down but since"
        }
      ]
    },
    {
      "tStartMs": 2376760,
      "dDurationMs": 3400,
      "segs": [
        {
          "utf8": "you just need to know their values relative to each other to pick the best"
        }
      ]
    },
    {
      "tStartMs": 2380160,
      "dDurationMs": 5840,
      "segs": [
        {
          "utf8": "one that's not necessarily an issue anyways the actual math that describes"
        }
      ]
    },
    {
      "tStartMs": 2386000,
      "dDurationMs": 6240,
      "segs": [
        {
          "utf8": "this relativity is like this if we write out a return as a discounted sum of"
        }
      ]
    },
    {
      "tStartMs": 2392240,
      "dDurationMs": 5359,
      "segs": [
        {
          "utf8": "rewards then we can see that we can take out the first reward and the rest of"
        }
      ]
    },
    {
      "tStartMs": 2397599,
      "dDurationMs": 5561,
      "segs": [
        {
          "utf8": "will just be the next return in the trajectory multiplied by gamma of course"
        }
      ]
    },
    {
      "tStartMs": 2403160,
      "dDurationMs": 6159,
      "segs": [
        {
          "utf8": "returns refer to within a specific trajectory so for our more General"
        }
      ]
    },
    {
      "tStartMs": 2409319,
      "dDurationMs": 6401,
      "segs": [
        {
          "utf8": "averaged out estimate for States and actions we can replace the GT with q and"
        }
      ]
    },
    {
      "tStartMs": 2415720,
      "dDurationMs": 7440,
      "segs": [
        {
          "utf8": "move our estimate towards this result by 10% for example like we did before now"
        }
      ]
    },
    {
      "tStartMs": 2423160,
      "dDurationMs": 4760,
      "segs": [
        {
          "utf8": "here for the next state and action we've just used the state and action that that"
        }
      ]
    },
    {
      "tStartMs": 2427920,
      "dDurationMs": 5080,
      "segs": [
        {
          "utf8": "happens to occur in the sample trajectory but since the environment has"
        }
      ]
    },
    {
      "tStartMs": 2433000,
      "dDurationMs": 4839,
      "segs": [
        {
          "utf8": "some Randomness this might not be the only subsequent State that's possible"
        }
      ]
    },
    {
      "tStartMs": 2437839,
      "dDurationMs": 4081,
      "segs": [
        {
          "utf8": "and since there's a range of different actions you can take this is also not"
        }
      ]
    },
    {
      "tStartMs": 2441920,
      "dDurationMs": 3800,
      "segs": [
        {
          "utf8": "the only subsequent action that's possible we can't do much about the"
        }
      ]
    },
    {
      "tStartMs": 2445720,
      "dDurationMs": 4680,
      "segs": [
        {
          "utf8": "state because we're talking about a model-free method right now so we don't"
        }
      ]
    },
    {
      "tStartMs": 2450400,
      "dDurationMs": 6280,
      "segs": [
        {
          "utf8": "have access to a world model and don't know the next state transitions but with"
        }
      ]
    },
    {
      "tStartMs": 2456680,
      "dDurationMs": 5439,
      "segs": [
        {
          "utf8": "the action as you may imagine there's a couple other ways that we can decide"
        }
      ]
    },
    {
      "tStartMs": 2462119,
      "dDurationMs": 4921,
      "segs": [
        {
          "utf8": "what action to put in here so that we can make this calculation for example"
        }
      ]
    },
    {
      "tStartMs": 2467040,
      "dDurationMs": 6520,
      "segs": [
        {
          "utf8": "you can account for all the possible actions averaged out according to the"
        }
      ]
    },
    {
      "tStartMs": 2473560,
      "dDurationMs": 6360,
      "segs": [
        {
          "utf8": "policy probabilities here it's a weighted sum across all the actions"
        }
      ]
    },
    {
      "tStartMs": 2479920,
      "dDurationMs": 5960,
      "segs": [
        {
          "utf8": "where you multiply by the probability or you could take the best action with the"
        }
      ]
    },
    {
      "tStartMs": 2485880,
      "dDurationMs": 4680,
      "segs": [
        {
          "utf8": "highest estimate rather than the one that you ended up taking here you're"
        }
      ]
    },
    {
      "tStartMs": 2490560,
      "dDurationMs": 6160,
      "segs": [
        {
          "utf8": "taking the maximum out of all the actions this algorithm is called sarsa"
        }
      ]
    },
    {
      "tStartMs": 2496720,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "this one is called expected sarsa and this one is called Q learning in other"
        }
      ]
    },
    {
      "tStartMs": 2502720,
      "dDurationMs": 7760,
      "segs": [
        {
          "utf8": "words for any given State and action Monte Carlo is saying what is my return"
        }
      ]
    },
    {
      "tStartMs": 2510480,
      "dDurationMs": 6800,
      "segs": [
        {
          "utf8": "sarsa is saying how much better or worse is my return relative to the new state"
        }
      ]
    },
    {
      "tStartMs": 2517280,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "and the action that I ended up taking expected sarsa is saying how much better"
        }
      ]
    },
    {
      "tStartMs": 2522720,
      "dDurationMs": 5320,
      "segs": [
        {
          "utf8": "or worse is my return relative to the new state with all of its actions"
        }
      ]
    },
    {
      "tStartMs": 2528040,
      "dDurationMs": 5520,
      "segs": [
        {
          "utf8": "averaged out according to policy probabilities Q learning is saying how"
        }
      ]
    },
    {
      "tStartMs": 2533560,
      "dDurationMs": 5200,
      "segs": [
        {
          "utf8": "much better or worse is my return relative to the new state with its best"
        }
      ]
    },
    {
      "tStartMs": 2538760,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "case scenario highest estimated action of course alongside these different ways"
        }
      ]
    },
    {
      "tStartMs": 2544760,
      "dDurationMs": 5559,
      "segs": [
        {
          "utf8": "of learning the action value Q function you can also learn the state value V"
        }
      ]
    },
    {
      "tStartMs": 2550319,
      "dDurationMs": 4961,
      "segs": [
        {
          "utf8": "function like so where it's a bit simpler because it only takes States as"
        }
      ]
    },
    {
      "tStartMs": 2555280,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "opposed to States and actions right now we're focusing on the Q function so this"
        }
      ]
    },
    {
      "tStartMs": 2560000,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "V function is not that important but do remember it because it'll come up later"
        }
      ]
    },
    {
      "tStartMs": 2564800,
      "dDurationMs": 4840,
      "segs": [
        {
          "utf8": "in the video furthermore I've written out the math in simplified form with an"
        }
      ]
    },
    {
      "tStartMs": 2569640,
      "dDurationMs": 5959,
      "segs": [
        {
          "utf8": "arrow and just described it as moving one value towards another value but"
        }
      ]
    },
    {
      "tStartMs": 2575599,
      "dDurationMs": 5561,
      "segs": [
        {
          "utf8": "mathematically it's like this where you assign to your estimate what it"
        }
      ]
    },
    {
      "tStartMs": 2581160,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "originally was plus the difference between the Target and the initial value"
        }
      ]
    },
    {
      "tStartMs": 2586440,
      "dDurationMs": 6720,
      "segs": [
        {
          "utf8": "multiplied by some sort of learning rate Alpha which we've used 0.14 up until now"
        }
      ]
    },
    {
      "tStartMs": 2593160,
      "dDurationMs": 5159,
      "segs": [
        {
          "utf8": "anyways that's just the actual math of it if you're curious now notice how a q"
        }
      ]
    },
    {
      "tStartMs": 2598319,
      "dDurationMs": 4721,
      "segs": [
        {
          "utf8": "function is usually supposed to have a pi attached to it because it's"
        }
      ]
    },
    {
      "tStartMs": 2603040,
      "dDurationMs": 4319,
      "segs": [
        {
          "utf8": "evaluating for that given policy but here for these different ways of"
        }
      ]
    },
    {
      "tStartMs": 2607359,
      "dDurationMs": 5081,
      "segs": [
        {
          "utf8": "constructing estimates I haven't put a pi on them and that's because they are"
        }
      ]
    },
    {
      "tStartMs": 2612440,
      "dDurationMs": 5800,
      "segs": [
        {
          "utf8": "estimating different things for sarsa and expected sarsa notice how we're"
        }
      ]
    },
    {
      "tStartMs": 2618240,
      "dDurationMs": 5240,
      "segs": [
        {
          "utf8": "constructing the estimates based on what action the policy ended up taking or"
        }
      ]
    },
    {
      "tStartMs": 2623480,
      "dDurationMs": 4920,
      "segs": [
        {
          "utf8": "what action it's expected to take based on the policy's probabilities in these"
        }
      ]
    },
    {
      "tStartMs": 2628400,
      "dDurationMs": 6560,
      "segs": [
        {
          "utf8": "two cases since the estimates are based on the policy Pi then it's estimating Q"
        }
      ]
    },
    {
      "tStartMs": 2634960,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "pi and then remember through this continuous cycle of estimating Q pi and"
        }
      ]
    },
    {
      "tStartMs": 2640400,
      "dDurationMs": 3919,
      "segs": [
        {
          "utf8": "then using it to improve pi and then estimating the new Q Pi again for the"
        }
      ]
    },
    {
      "tStartMs": 2644319,
      "dDurationMs": 5800,
      "segs": [
        {
          "utf8": "improved Pi called generalized policy iteration eventually Q Pi will become"
        }
      ]
    },
    {
      "tStartMs": 2650119,
      "dDurationMs": 6480,
      "segs": [
        {
          "utf8": "the optimal Q function qar and Pi will become the optimal policy but for"
        }
      ]
    },
    {
      "tStartMs": 2656599,
      "dDurationMs": 4161,
      "segs": [
        {
          "utf8": "q-learning if we're constructing the estimate based on the best action to"
        }
      ]
    },
    {
      "tStartMs": 2660760,
      "dDurationMs": 5400,
      "segs": [
        {
          "utf8": "take in the next state then is no longer dependent on any policy like the other"
        }
      ]
    },
    {
      "tStartMs": 2666160,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "two that means instead of estimating Q Pi which will hopefully eventually"
        }
      ]
    },
    {
      "tStartMs": 2670880,
      "dDurationMs": 6640,
      "segs": [
        {
          "utf8": "become qar it instead directly estimates qar so there is a terminology to"
        }
      ]
    },
    {
      "tStartMs": 2677520,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "describe this difference the first two are called on policy methods where the"
        }
      ]
    },
    {
      "tStartMs": 2683520,
      "dDurationMs": 6200,
      "segs": [
        {
          "utf8": "behavior policy the policy used to gather experiences is the same as the"
        }
      ]
    },
    {
      "tStartMs": 2689720,
      "dDurationMs": 6160,
      "segs": [
        {
          "utf8": "target policy the policy that is being evaluated or improved these two are"
        }
      ]
    },
    {
      "tStartMs": 2695880,
      "dDurationMs": 3520,
      "segs": [
        {
          "utf8": "evaluating part which is the policy that's used to"
        }
      ]
    },
    {
      "tStartMs": 2699400,
      "dDurationMs": 4959,
      "segs": [
        {
          "utf8": "gather the experiences and trajectories in the first place the last one is"
        }
      ]
    },
    {
      "tStartMs": 2704359,
      "dDurationMs": 5681,
      "segs": [
        {
          "utf8": "called an off policy method where the behavior policy is not the same as the"
        }
      ]
    },
    {
      "tStartMs": 2710040,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "target policy it still uses the policy pi to gather experiences but it's not"
        }
      ]
    },
    {
      "tStartMs": 2715480,
      "dDurationMs": 4920,
      "segs": [
        {
          "utf8": "trying to evaluate the policy Pi it's instead trying to evaluate the optimal"
        }
      ]
    },
    {
      "tStartMs": 2720400,
      "dDurationMs": 6719,
      "segs": [
        {
          "utf8": "policy directly so Monte Carlo saraw expected saraw and Q learning all these"
        }
      ]
    },
    {
      "tStartMs": 2727119,
      "dDurationMs": 4641,
      "segs": [
        {
          "utf8": "methods differ in their sample efficiency what that means is the number"
        }
      ]
    },
    {
      "tStartMs": 2731760,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "of samples either the number of episodes or the number of time steps that it"
        }
      ]
    },
    {
      "tStartMs": 2736400,
      "dDurationMs": 5400,
      "segs": [
        {
          "utf8": "takes to get good at a task if for the same task one method requires 100"
        }
      ]
    },
    {
      "tStartMs": 2741800,
      "dDurationMs": 4759,
      "segs": [
        {
          "utf8": "episodes to get good at it and another method requires only 50 episodes then"
        }
      ]
    },
    {
      "tStartMs": 2746559,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "the 50 episode method is more sample efficient and that's what we want"
        }
      ]
    },
    {
      "tStartMs": 2751599,
      "dDurationMs": 5000,
      "segs": [
        {
          "utf8": "usually Monte Carlo is the least sample efficient because the other three have"
        }
      ]
    },
    {
      "tStartMs": 2756599,
      "dDurationMs": 5041,
      "segs": [
        {
          "utf8": "taken advantage of temporal difference and it has not then it's usually"
        }
      ]
    },
    {
      "tStartMs": 2761640,
      "dDurationMs": 5000,
      "segs": [
        {
          "utf8": "followed by sarsa which is more sample efficient and then expected sarsa which"
        }
      ]
    },
    {
      "tStartMs": 2766640,
      "dDurationMs": 5160,
      "segs": [
        {
          "utf8": "is even more sample efficient and then finally Q learning which is the most"
        }
      ]
    },
    {
      "tStartMs": 2771800,
      "dDurationMs": 4680,
      "segs": [
        {
          "utf8": "sample efficient because it has taken advantage of both temporal difference"
        }
      ]
    },
    {
      "tStartMs": 2776480,
      "dDurationMs": 6119,
      "segs": [
        {
          "utf8": "and of policy learning now off policy has its advantages and disadvantages but"
        }
      ]
    },
    {
      "tStartMs": 2782599,
      "dDurationMs": 4601,
      "segs": [
        {
          "utf8": "this sample efficiency is sometimes one of the advantages of off po policy"
        }
      ]
    },
    {
      "tStartMs": 2787200,
      "dDurationMs": 4600,
      "segs": [
        {
          "utf8": "learning now we've said that Q learning tries to estimate the optimal qar"
        }
      ]
    },
    {
      "tStartMs": 2791800,
      "dDurationMs": 5080,
      "segs": [
        {
          "utf8": "function through updating like this there's an equation that describes what"
        }
      ]
    },
    {
      "tStartMs": 2796880,
      "dDurationMs": 4760,
      "segs": [
        {
          "utf8": "this optimal qar function will actually end up being remember how the"
        }
      ]
    },
    {
      "tStartMs": 2801640,
      "dDurationMs": 4679,
      "segs": [
        {
          "utf8": "environment is random and after you take a certain action you won't always end up"
        }
      ]
    },
    {
      "tStartMs": 2806319,
      "dDurationMs": 4841,
      "segs": [
        {
          "utf8": "getting the exact same reward or the exact same next state and this is of"
        }
      ]
    },
    {
      "tStartMs": 2811160,
      "dDurationMs": 5000,
      "segs": [
        {
          "utf8": "course described by the world model function that gives the probability of"
        }
      ]
    },
    {
      "tStartMs": 2816160,
      "dDurationMs": 5159,
      "segs": [
        {
          "utf8": "the different reward Wards and next states which we don't have access to"
        }
      ]
    },
    {
      "tStartMs": 2821319,
      "dDurationMs": 5481,
      "segs": [
        {
          "utf8": "well if you do q-learning with enough sample trajectories it will average out"
        }
      ]
    },
    {
      "tStartMs": 2826800,
      "dDurationMs": 4799,
      "segs": [
        {
          "utf8": "and you will end up approximating this optimal qar function which is supposed"
        }
      ]
    },
    {
      "tStartMs": 2831599,
      "dDurationMs": 4281,
      "segs": [
        {
          "utf8": "to be the expected value of this relationship we have been trying to"
        }
      ]
    },
    {
      "tStartMs": 2835880,
      "dDurationMs": 5320,
      "segs": [
        {
          "utf8": "capture the expected value is just a weighted average similar to an expected"
        }
      ]
    },
    {
      "tStartMs": 2841200,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "sarsa but this time according to the probabilities in the environment meaning"
        }
      ]
    },
    {
      "tStartMs": 2846480,
      "dDurationMs": 4839,
      "segs": [
        {
          "utf8": "that even though we don't have access to this world model the samples we collect"
        }
      ]
    },
    {
      "tStartMs": 2851319,
      "dDurationMs": 5240,
      "segs": [
        {
          "utf8": "will still average out according to it furthermore aside from this expected"
        }
      ]
    },
    {
      "tStartMs": 2856559,
      "dDurationMs": 6201,
      "segs": [
        {
          "utf8": "value the expression inside it expresses a property called belman's principle of"
        }
      ]
    },
    {
      "tStartMs": 2862760,
      "dDurationMs": 4240,
      "segs": [
        {
          "utf8": "optimality which if you look on Wikipedia is a quote from some guy"
        }
      ]
    },
    {
      "tStartMs": 2867000,
      "dDurationMs": 4480,
      "segs": [
        {
          "utf8": "called Richard Bellman who was figuring out some of this math back in the"
        }
      ]
    },
    {
      "tStartMs": 2871480,
      "dDurationMs": 5359,
      "segs": [
        {
          "utf8": "1950s it says an optimal policy has the property that whatever the initial state"
        }
      ]
    },
    {
      "tStartMs": 2876839,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "and initial decision are the remaining decisions must constitute an optimal"
        }
      ]
    },
    {
      "tStartMs": 2881559,
      "dDurationMs": 4280,
      "segs": [
        {
          "utf8": "policy with regard to the state resulting from the first decision what"
        }
      ]
    },
    {
      "tStartMs": 2885839,
      "dDurationMs": 4561,
      "segs": [
        {
          "utf8": "this means is if there's a series of decisions which is optimal as a whole"
        }
      ]
    },
    {
      "tStartMs": 2890400,
      "dDurationMs": 4520,
      "segs": [
        {
          "utf8": "then if you look at all the decisions except for the first one those have to"
        }
      ]
    },
    {
      "tStartMs": 2894920,
      "dDurationMs": 4919,
      "segs": [
        {
          "utf8": "be optimal too which kind of makes sense right if you want to get full marks in a"
        }
      ]
    },
    {
      "tStartMs": 2899839,
      "dDurationMs": 4801,
      "segs": [
        {
          "utf8": "test and the test is split into three sections then naturally that means you"
        }
      ]
    },
    {
      "tStartMs": 2904640,
      "dDurationMs": 4600,
      "segs": [
        {
          "utf8": "must get full marks in each individual section it sounds pretty obvious but"
        }
      ]
    },
    {
      "tStartMs": 2909240,
      "dDurationMs": 4319,
      "segs": [
        {
          "utf8": "back then this was groundbreaking stuff so that's kind of the theory behind"
        }
      ]
    },
    {
      "tStartMs": 2913559,
      "dDurationMs": 5321,
      "segs": [
        {
          "utf8": "putting a Max around this subsequent bit in order to find the optimal Q function"
        }
      ]
    },
    {
      "tStartMs": 2918880,
      "dDurationMs": 4760,
      "segs": [
        {
          "utf8": "and this is called a Bellman optimality equation there is a Bellman optimality"
        }
      ]
    },
    {
      "tStartMs": 2923640,
      "dDurationMs": 5520,
      "segs": [
        {
          "utf8": "equation for the other value function v as well but we're focusing on Q for now"
        }
      ]
    },
    {
      "tStartMs": 2929160,
      "dDurationMs": 4520,
      "segs": [
        {
          "utf8": "I don't think it's too important to know the historical context and terminology"
        }
      ]
    },
    {
      "tStartMs": 2933680,
      "dDurationMs": 4760,
      "segs": [
        {
          "utf8": "for this bit of math but everyone who teaches RL mentions it so I thought I'd"
        }
      ]
    },
    {
      "tStartMs": 2938440,
      "dDurationMs": 4159,
      "segs": [
        {
          "utf8": "include it as well to quickly demonstrate how Q Learning Works in"
        }
      ]
    },
    {
      "tStartMs": 2942599,
      "dDurationMs": 4200,
      "segs": [
        {
          "utf8": "practice here's the table again with some random numbers filled in let's say"
        }
      ]
    },
    {
      "tStartMs": 2946799,
      "dDurationMs": 5961,
      "segs": [
        {
          "utf8": "you're in this state picked this action and then ended up in this state with a"
        }
      ]
    },
    {
      "tStartMs": 2952760,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "reward of minus1 then you'd simply look at the new state find the maximum value"
        }
      ]
    },
    {
      "tStartMs": 2958040,
      "dDurationMs": 4840,
      "segs": [
        {
          "utf8": "out of all the actions there which is this one multiply by gamma and then add"
        }
      ]
    },
    {
      "tStartMs": 2962880,
      "dDurationMs": 6080,
      "segs": [
        {
          "utf8": "on the reward once you have that result you simply go back to the original state"
        }
      ]
    },
    {
      "tStartMs": 2968960,
      "dDurationMs": 7080,
      "segs": [
        {
          "utf8": "and action and then move that value towards it by a learning rate of 10% for"
        }
      ]
    },
    {
      "tStartMs": 2976040,
      "dDurationMs": 4880,
      "segs": [
        {
          "utf8": "example this way you only need the reward for one time step and you don't"
        }
      ]
    },
    {
      "tStartMs": 2980920,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "have to add up all the rewards in a trajectory so let's run the simulation"
        }
      ]
    },
    {
      "tStartMs": 2985720,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "again with Monte Carlo and Q learning side by side and see if it has any"
        }
      ]
    },
    {
      "tStartMs": 2990680,
      "dDurationMs": 4639,
      "segs": [
        {
          "utf8": "Improvement here I've made it so they both use the same Epsilon they both use"
        }
      ]
    },
    {
      "tStartMs": 2995319,
      "dDurationMs": 6800,
      "segs": [
        {
          "utf8": "the same gamma of 0. 8 and they both use the same learn rate of 0.1 or 10% for"
        }
      ]
    },
    {
      "tStartMs": 3002119,
      "dDurationMs": 4521,
      "segs": [
        {
          "utf8": "this small maze there is a bit of an improvements but the difference is not"
        }
      ]
    },
    {
      "tStartMs": 3006640,
      "dDurationMs": 4760,
      "segs": [
        {
          "utf8": "that big if we switch to a bigger maze though then immediately you can see that"
        }
      ]
    },
    {
      "tStartMs": 3011400,
      "dDurationMs": 4880,
      "segs": [
        {
          "utf8": "Q learning is doing way better and Monte Carlo is never able to reach the target"
        }
      ]
    },
    {
      "tStartMs": 3016280,
      "dDurationMs": 4839,
      "segs": [
        {
          "utf8": "Square even once the benefits of a more efficient algorithm really start to show"
        }
      ]
    },
    {
      "tStartMs": 3021119,
      "dDurationMs": 3561,
      "segs": [
        {
          "utf8": "in more complex environments now you might be thinking"
        }
      ]
    },
    {
      "tStartMs": 3024680,
      "dDurationMs": 3639,
      "segs": [
        {
          "utf8": "that this is only because Monte Carlo was was never able to reach the target"
        }
      ]
    },
    {
      "tStartMs": 3028319,
      "dDurationMs": 5321,
      "segs": [
        {
          "utf8": "even once and it just got unlucky so to see what would happen if Monte Carlo did"
        }
      ]
    },
    {
      "tStartMs": 3033640,
      "dDurationMs": 4919,
      "segs": [
        {
          "utf8": "have some successful experiences I did another run where for the first 50"
        }
      ]
    },
    {
      "tStartMs": 3038559,
      "dDurationMs": 4961,
      "segs": [
        {
          "utf8": "episodes it was artificially helped to move to the Target as efficiently as"
        }
      ]
    },
    {
      "tStartMs": 3043520,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "possible but even after that Monte Carlo failed to reach the target again in the"
        }
      ]
    },
    {
      "tStartMs": 3048240,
      "dDurationMs": 5960,
      "segs": [
        {
          "utf8": "rest of the run the point is Q learning is better"
        }
      ]
    },
    {
      "tStartMs": 3055360,
      "dDurationMs": 3089,
      "segs": [
        {
          "utf8": "[Music]"
        }
      ]
    },
    {
      "tStartMs": 3059799,
      "dDurationMs": 4601,
      "segs": [
        {
          "utf8": "so now that we've gotten up to Q learning which is more sample efficient"
        }
      ]
    },
    {
      "tStartMs": 3064400,
      "dDurationMs": 5520,
      "segs": [
        {
          "utf8": "than the other ones we have finally gotten good enough at the fundamentals"
        }
      ]
    },
    {
      "tStartMs": 3069920,
      "dDurationMs": 7159,
      "segs": [
        {
          "utf8": "that we can slap on a neural network and move past this grid example the main"
        }
      ]
    },
    {
      "tStartMs": 3077079,
      "dDurationMs": 5561,
      "segs": [
        {
          "utf8": "benefit of neural networks is that we can go from discrete States and actions"
        }
      ]
    },
    {
      "tStartMs": 3082640,
      "dDurationMs": 5520,
      "segs": [
        {
          "utf8": "to continuous States and actions in other words instead of only having whole"
        }
      ]
    },
    {
      "tStartMs": 3088160,
      "dDurationMs": 5480,
      "segs": [
        {
          "utf8": "number positions and a limited number of actions in this grid neural networks can"
        }
      ]
    },
    {
      "tStartMs": 3093640,
      "dDurationMs": 5880,
      "segs": [
        {
          "utf8": "allow for an infinite continuous range of states and actions as you can imagine"
        }
      ]
    },
    {
      "tStartMs": 3099520,
      "dDurationMs": 5319,
      "segs": [
        {
          "utf8": "this is necessary for basically any real world physical task where you're dealing"
        }
      ]
    },
    {
      "tStartMs": 3104839,
      "dDurationMs": 5401,
      "segs": [
        {
          "utf8": "with things like angles velocities positions and directions it's impossible"
        }
      ]
    },
    {
      "tStartMs": 3110240,
      "dDurationMs": 5079,
      "segs": [
        {
          "utf8": "to use a table for continuous States and actions because you can't have an"
        }
      ]
    },
    {
      "tStartMs": 3115319,
      "dDurationMs": 3561,
      "segs": [
        {
          "utf8": "infinitely big table the first algorithm we'll look at is"
        }
      ]
    },
    {
      "tStartMs": 3118880,
      "dDurationMs": 7040,
      "segs": [
        {
          "utf8": "called Deep Q Network or dqn which takes us halfway there as in"
        }
      ]
    },
    {
      "tStartMs": 3125920,
      "dDurationMs": 5480,
      "segs": [
        {
          "utf8": "it can allow for continuous States but not continuous actions so we'll try and"
        }
      ]
    },
    {
      "tStartMs": 3131400,
      "dDurationMs": 4320,
      "segs": [
        {
          "utf8": "solve a task called lunar lander where you're supposed to activate the engines"
        }
      ]
    },
    {
      "tStartMs": 3135720,
      "dDurationMs": 5399,
      "segs": [
        {
          "utf8": "on this 2D spaceship and try to land it in the middle the task is designed so"
        }
      ]
    },
    {
      "tStartMs": 3141119,
      "dDurationMs": 4401,
      "segs": [
        {
          "utf8": "that there's three engines which are either on or off and you can only turn"
        }
      ]
    },
    {
      "tStartMs": 3145520,
      "dDurationMs": 5079,
      "segs": [
        {
          "utf8": "on one at a time which means there's four discrete actions one for having no"
        }
      ]
    },
    {
      "tStartMs": 3150599,
      "dDurationMs": 5520,
      "segs": [
        {
          "utf8": "engines on and one for each engine the reason deep Q networks only allow for a"
        }
      ]
    },
    {
      "tStartMs": 3156119,
      "dDurationMs": 5561,
      "segs": [
        {
          "utf8": "limited discrete number of actions is because it's a value based method"
        }
      ]
    },
    {
      "tStartMs": 3161680,
      "dDurationMs": 4399,
      "segs": [
        {
          "utf8": "remember how there is methods that are more reliant on the policy function"
        }
      ]
    },
    {
      "tStartMs": 3166079,
      "dDurationMs": 4881,
      "segs": [
        {
          "utf8": "methods that are more reliant on value functions and methods that are a balance"
        }
      ]
    },
    {
      "tStartMs": 3170960,
      "dDurationMs": 5720,
      "segs": [
        {
          "utf8": "between the two up till now we've been focusing on the last category where"
        }
      ]
    },
    {
      "tStartMs": 3176680,
      "dDurationMs": 5840,
      "segs": [
        {
          "utf8": "Monte Carlo sarsa expected sarsa and Q learning are all value-based methods"
        }
      ]
    },
    {
      "tStartMs": 3182520,
      "dDurationMs": 5559,
      "segs": [
        {
          "utf8": "they all evaluate actions so obviously you can't just evaluate an infinite"
        }
      ]
    },
    {
      "tStartMs": 3188079,
      "dDurationMs": 4601,
      "segs": [
        {
          "utf8": "number of actions in order to pick the best one for continuous actions we have"
        }
      ]
    },
    {
      "tStartMs": 3192680,
      "dDurationMs": 5320,
      "segs": [
        {
          "utf8": "to go back to policy gradients methods which we'll look at later anyways with"
        }
      ]
    },
    {
      "tStartMs": 3198000,
      "dDurationMs": 4920,
      "segs": [
        {
          "utf8": "deep Q networks you basically take Q learning but swap out the table for a"
        }
      ]
    },
    {
      "tStartMs": 3202920,
      "dDurationMs": 4080,
      "segs": [
        {
          "utf8": "neural network the table takes input States and AC"
        }
      ]
    },
    {
      "tStartMs": 3207000,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "and outputs and evaluation and you can go and adjust its outputs over time well"
        }
      ]
    },
    {
      "tStartMs": 3213000,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "a neural network can also take input States and actions and output and"
        }
      ]
    },
    {
      "tStartMs": 3218040,
      "dDurationMs": 5079,
      "segs": [
        {
          "utf8": "evaluations and then you'd go and adjust its outputs using back propagation and"
        }
      ]
    },
    {
      "tStartMs": 3223119,
      "dDurationMs": 5720,
      "segs": [
        {
          "utf8": "gradient descent well with a slight adjustment you take an input State and"
        }
      ]
    },
    {
      "tStartMs": 3228839,
      "dDurationMs": 4681,
      "segs": [
        {
          "utf8": "output evaluations for all the actions simultaneously because that's more"
        }
      ]
    },
    {
      "tStartMs": 3233520,
      "dDurationMs": 3680,
      "segs": [
        {
          "utf8": "efficient the neural network is a function"
        }
      ]
    },
    {
      "tStartMs": 3237200,
      "dDurationMs": 4760,
      "segs": [
        {
          "utf8": "approximator because while with the table you can keep track of a whole"
        }
      ]
    },
    {
      "tStartMs": 3241960,
      "dDurationMs": 5680,
      "segs": [
        {
          "utf8": "bunch of exact values which don't affect each other with a neural network you"
        }
      ]
    },
    {
      "tStartMs": 3247640,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "have a limited number of parameters which affect how all the outputs are"
        }
      ]
    },
    {
      "tStartMs": 3252680,
      "dDurationMs": 5720,
      "segs": [
        {
          "utf8": "calculated so here is lunar lander with a neural network instead of a table to"
        }
      ]
    },
    {
      "tStartMs": 3258400,
      "dDurationMs": 5399,
      "segs": [
        {
          "utf8": "approximate its Q function the neural network has layers of 64 nodes each in"
        }
      ]
    },
    {
      "tStartMs": 3263799,
      "dDurationMs": 3241,
      "segs": [
        {
          "utf8": "the middle so I wasn't able to show all of them"
        }
      ]
    },
    {
      "tStartMs": 3267040,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "but it's essentially updating according to Q learning so it's trying to estimate"
        }
      ]
    },
    {
      "tStartMs": 3271680,
      "dDurationMs": 6439,
      "segs": [
        {
          "utf8": "the optimal qar function in a way that is off policy making it more sample"
        }
      ]
    },
    {
      "tStartMs": 3278119,
      "dDurationMs": 5480,
      "segs": [
        {
          "utf8": "efficient compared to the grid this environment uses much smaller time steps"
        }
      ]
    },
    {
      "tStartMs": 3283599,
      "dDurationMs": 5441,
      "segs": [
        {
          "utf8": "and much more time steps so we are using a gamma discount factor of"
        }
      ]
    },
    {
      "tStartMs": 3289040,
      "dDurationMs": 6400,
      "segs": [
        {
          "utf8": "0.999 and a learn rate of 0.001 as you can see we have our"
        }
      ]
    },
    {
      "tStartMs": 3295440,
      "dDurationMs": 4280,
      "segs": [
        {
          "utf8": "familiar Epsilon here which dictates the probability that it'll take random"
        }
      ]
    },
    {
      "tStartMs": 3299720,
      "dDurationMs": 4639,
      "segs": [
        {
          "utf8": "actions as opposed to the most optimal one this environment comes from a python"
        }
      ]
    },
    {
      "tStartMs": 3304359,
      "dDurationMs": 6521,
      "segs": [
        {
          "utf8": "Library called open AI gym and the state consists of eight variables the X and Y"
        }
      ]
    },
    {
      "tStartMs": 3310880,
      "dDurationMs": 6160,
      "segs": [
        {
          "utf8": "coordinates of the spaceship the X and Y velocities of the spaceship the angle"
        }
      ]
    },
    {
      "tStartMs": 3317040,
      "dDurationMs": 5759,
      "segs": [
        {
          "utf8": "the angular velocity which is how fast it's spinning and two yes or no values"
        }
      ]
    },
    {
      "tStartMs": 3322799,
      "dDurationMs": 4401,
      "segs": [
        {
          "utf8": "saying whether its two legs are in contact with the ground the rewards at"
        }
      ]
    },
    {
      "tStartMs": 3327200,
      "dDurationMs": 4520,
      "segs": [
        {
          "utf8": "every time step will be higher if the spaceship is closer to The Landing Pad"
        }
      ]
    },
    {
      "tStartMs": 3331720,
      "dDurationMs": 5480,
      "segs": [
        {
          "utf8": "moving more slowly and in a more horizontal position whenever the engines"
        }
      ]
    },
    {
      "tStartMs": 3337200,
      "dDurationMs": 4599,
      "segs": [
        {
          "utf8": "fire it results in a small penalty in the reward to try to get the spaceship"
        }
      ]
    },
    {
      "tStartMs": 3341799,
      "dDurationMs": 4481,
      "segs": [
        {
          "utf8": "to use less Fuel and there's a bigger reward when the legs are in contact with"
        }
      ]
    },
    {
      "tStartMs": 3346280,
      "dDurationMs": 5039,
      "segs": [
        {
          "utf8": "the ground finally there's a massive penalty if it crashes and a massive"
        }
      ]
    },
    {
      "tStartMs": 3351319,
      "dDurationMs": 5081,
      "segs": [
        {
          "utf8": "reward if it lands safely of course at the start the untrained spaceship will"
        }
      ]
    },
    {
      "tStartMs": 3356400,
      "dDurationMs": 4840,
      "segs": [
        {
          "utf8": "keep crashing but over time it does improve now there's actually many"
        }
      ]
    },
    {
      "tStartMs": 3361240,
      "dDurationMs": 6520,
      "segs": [
        {
          "utf8": "variants of DQ networks also known as DQ NS there's double dqn dueling dqn"
        }
      ]
    },
    {
      "tStartMs": 3367760,
      "dDurationMs": 5160,
      "segs": [
        {
          "utf8": "rainbow dqn Etc these algorithms all differ in the little tricks that they"
        }
      ]
    },
    {
      "tStartMs": 3372920,
      "dDurationMs": 4080,
      "segs": [
        {
          "utf8": "use but fundamentally they're all basically taking Q learning and"
        }
      ]
    },
    {
      "tStartMs": 3377000,
      "dDurationMs": 5880,
      "segs": [
        {
          "utf8": "replacing the table with neural networks in fact even this normal dqn which came"
        }
      ]
    },
    {
      "tStartMs": 3382880,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "out in 2013 in a paper called playing Atari with deep reinforcement learning"
        }
      ]
    },
    {
      "tStartMs": 3388240,
      "dDurationMs": 6400,
      "segs": [
        {
          "utf8": "uses a couple of these tricks such as a Target Network and a replay buffer but"
        }
      ]
    },
    {
      "tStartMs": 3394640,
      "dDurationMs": 4360,
      "segs": [
        {
          "utf8": "that's outside the scope of this video we won't go into all of those tricks"
        }
      ]
    },
    {
      "tStartMs": 3399000,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "after around 100 episodes the spaceship has gotten pretty good and pretty"
        }
      ]
    },
    {
      "tStartMs": 3403720,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "consistent as well it doesn't crash anymore it stays upright and whenever it"
        }
      ]
    },
    {
      "tStartMs": 3409720,
      "dDurationMs": 4520,
      "segs": [
        {
          "utf8": "veers off to the side because there's a random Force that's applied to it at the"
        }
      ]
    },
    {
      "tStartMs": 3414240,
      "dDurationMs": 4359,
      "segs": [
        {
          "utf8": "start of every episode to make it more challenging it's able to slowly readjust"
        }
      ]
    },
    {
      "tStartMs": 3418599,
      "dDurationMs": 4281,
      "segs": [
        {
          "utf8": "and move back into the middle before landing so let's try and visualize what"
        }
      ]
    },
    {
      "tStartMs": 3422880,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "the network has learned here are some examples of outputs from a network"
        }
      ]
    },
    {
      "tStartMs": 3427520,
      "dDurationMs": 4240,
      "segs": [
        {
          "utf8": "trained on lunar lander I've laid it out so that this rectangular area"
        }
      ]
    },
    {
      "tStartMs": 3431760,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "corresponds to the X and Y coordinates of the spaceship which is the first two"
        }
      ]
    },
    {
      "tStartMs": 3436480,
      "dDurationMs": 4000,
      "segs": [
        {
          "utf8": "numbers in the state and the other numbers in the States can be altered"
        }
      ]
    },
    {
      "tStartMs": 3440480,
      "dDurationMs": 4440,
      "segs": [
        {
          "utf8": "down here using sliders as you can see it's been trained to the point where you"
        }
      ]
    },
    {
      "tStartMs": 3444920,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "can understand the outputs reasonably well so when the spaceship is not moving"
        }
      ]
    },
    {
      "tStartMs": 3449880,
      "dDurationMs": 4919,
      "segs": [
        {
          "utf8": "completely horizontal and not rotating then it will not fire any engine if it's"
        }
      ]
    },
    {
      "tStartMs": 3454799,
      "dDurationMs": 3921,
      "segs": [
        {
          "utf8": "right above the target when it's off to the left it'll fire its left engine to"
        }
      ]
    },
    {
      "tStartMs": 3458720,
      "dDurationMs": 3599,
      "segs": [
        {
          "utf8": "move back towards the right when it's off to the right it'll fire Its Right"
        }
      ]
    },
    {
      "tStartMs": 3462319,
      "dDurationMs": 4121,
      "segs": [
        {
          "utf8": "engine to move back towards the left and if it's too far down it'll fire its down"
        }
      ]
    },
    {
      "tStartMs": 3466440,
      "dDurationMs": 4639,
      "segs": [
        {
          "utf8": "engine to come back up other than that most things make sense but not"
        }
      ]
    },
    {
      "tStartMs": 3471079,
      "dDurationMs": 5161,
      "segs": [
        {
          "utf8": "everything if the x velocity is positive so it's moving to the right it's more"
        }
      ]
    },
    {
      "tStartMs": 3476240,
      "dDurationMs": 5119,
      "segs": [
        {
          "utf8": "likely to fire Its Right engine to move back left but if it's moving to the left"
        }
      ]
    },
    {
      "tStartMs": 3481359,
      "dDurationMs": 2881,
      "segs": [
        {
          "utf8": "for some reason the left engine is not fired more"
        }
      ]
    },
    {
      "tStartMs": 3484240,
      "dDurationMs": 5119,
      "segs": [
        {
          "utf8": "frequently if it's got a negative y velocity meaning it's moving downwards"
        }
      ]
    },
    {
      "tStartMs": 3489359,
      "dDurationMs": 4801,
      "segs": [
        {
          "utf8": "it's more likely to fire its down engine to slow down but for some reason if it's"
        }
      ]
    },
    {
      "tStartMs": 3494160,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "moving upwards it'll start firing Its Right engine if the angle is positive"
        }
      ]
    },
    {
      "tStartMs": 3499520,
      "dDurationMs": 2960,
      "segs": [
        {
          "utf8": "meaning counterclockwise it'll fire the left"
        }
      ]
    },
    {
      "tStartMs": 3502480,
      "dDurationMs": 5119,
      "segs": [
        {
          "utf8": "engine more and if it's angled clockwise it'll fire the right engine more the"
        }
      ]
    },
    {
      "tStartMs": 3507599,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "same thing can be seen for angular velocity which is how fast it's rotating"
        }
      ]
    },
    {
      "tStartMs": 3512319,
      "dDurationMs": 5240,
      "segs": [
        {
          "utf8": "for comparison I've also visualized a random untrained Network and it's much"
        }
      ]
    },
    {
      "tStartMs": 3517559,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "harder to see a pattern or explanation in how its inputs relate to its"
        }
      ]
    },
    {
      "tStartMs": 3526260,
      "dDurationMs": 3079,
      "segs": [
        {
          "utf8": "[Music]"
        }
      ]
    },
    {
      "tStartMs": 3529839,
      "dDurationMs": 5361,
      "segs": [
        {
          "utf8": "outputs now let's talk about policy gradients if you'll remember from before"
        }
      ]
    },
    {
      "tStartMs": 3535200,
      "dDurationMs": 4440,
      "segs": [
        {
          "utf8": "this is where we push up the probabilities for actions that did well"
        }
      ]
    },
    {
      "tStartMs": 3539640,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "and push down the probabilities for actions that did poorly here instead of"
        }
      ]
    },
    {
      "tStartMs": 3544440,
      "dDurationMs": 5480,
      "segs": [
        {
          "utf8": "just focusing on evaluation with a value function and having the improvements of"
        }
      ]
    },
    {
      "tStartMs": 3549920,
      "dDurationMs": 4919,
      "segs": [
        {
          "utf8": "the policy be just based on that we are now separately keeping track of a"
        }
      ]
    },
    {
      "tStartMs": 3554839,
      "dDurationMs": 5681,
      "segs": [
        {
          "utf8": "distribution of probabilities and altering it at will furthermore it turns"
        }
      ]
    },
    {
      "tStartMs": 3560520,
      "dDurationMs": 4680,
      "segs": [
        {
          "utf8": "out that not only can you have a discrete probability distribution you"
        }
      ]
    },
    {
      "tStartMs": 3565200,
      "dDurationMs": 5000,
      "segs": [
        {
          "utf8": "can also alter policy gradients to work with A continuous probability"
        }
      ]
    },
    {
      "tStartMs": 3570200,
      "dDurationMs": 4119,
      "segs": [
        {
          "utf8": "distribution defined by a mean and standard deviation for example which"
        }
      ]
    },
    {
      "tStartMs": 3574319,
      "dDurationMs": 4361,
      "segs": [
        {
          "utf8": "we'll look at later this is of course important for many tasks where there is"
        }
      ]
    },
    {
      "tStartMs": 3578680,
      "dDurationMs": 4520,
      "segs": [
        {
          "utf8": "a physical aspect where it's not feasible to classify actions into"
        }
      ]
    },
    {
      "tStartMs": 3583200,
      "dDurationMs": 4399,
      "segs": [
        {
          "utf8": "discret options so let's start off by talking about a fundamental difference"
        }
      ]
    },
    {
      "tStartMs": 3587599,
      "dDurationMs": 6520,
      "segs": [
        {
          "utf8": "between learning a better value function for evaluation versus learning a better"
        }
      ]
    },
    {
      "tStartMs": 3594119,
      "dDurationMs": 5121,
      "segs": [
        {
          "utf8": "policy function for Improv movement if you'll notice whenever we have learned a"
        }
      ]
    },
    {
      "tStartMs": 3599240,
      "dDurationMs": 4480,
      "segs": [
        {
          "utf8": "value function up until now no matter if it's with a table or with a neural"
        }
      ]
    },
    {
      "tStartMs": 3603720,
      "dDurationMs": 6040,
      "segs": [
        {
          "utf8": "network no matter if it's Monte Carlo or Sara or expected sarsa or Q learning"
        }
      ]
    },
    {
      "tStartMs": 3609760,
      "dDurationMs": 4599,
      "segs": [
        {
          "utf8": "there's always some sort of Target that we calculate and then try to get the"
        }
      ]
    },
    {
      "tStartMs": 3614359,
      "dDurationMs": 5161,
      "segs": [
        {
          "utf8": "function to move closer to and this perfectly fits how a neural network"
        }
      ]
    },
    {
      "tStartMs": 3619520,
      "dDurationMs": 4559,
      "segs": [
        {
          "utf8": "traditionally functions where you're trying to use gradient descent to"
        }
      ]
    },
    {
      "tStartMs": 3624079,
      "dDurationMs": 6401,
      "segs": [
        {
          "utf8": "minimize a loss function in order to get closer to the targets or ground truth"
        }
      ]
    },
    {
      "tStartMs": 3630480,
      "dDurationMs": 6160,
      "segs": [
        {
          "utf8": "that you give to the network however in order to improve the policy function"
        }
      ]
    },
    {
      "tStartMs": 3636640,
      "dDurationMs": 6080,
      "segs": [
        {
          "utf8": "with all the probabilities that it keeps track of what's the target if we're just"
        }
      ]
    },
    {
      "tStartMs": 3642720,
      "dDurationMs": 4639,
      "segs": [
        {
          "utf8": "adjusting probabilities up and down depending on whether the action did well"
        }
      ]
    },
    {
      "tStartMs": 3647359,
      "dDurationMs": 4321,
      "segs": [
        {
          "utf8": "then that's more akin to trying out different things and seeing what works"
        }
      ]
    },
    {
      "tStartMs": 3651680,
      "dDurationMs": 6960,
      "segs": [
        {
          "utf8": "there is no such clearcut Target without a ground truth Target we no longer have"
        }
      ]
    },
    {
      "tStartMs": 3658640,
      "dDurationMs": 6560,
      "segs": [
        {
          "utf8": "a loss function to minimize without something to minimize we are instead"
        }
      ]
    },
    {
      "tStartMs": 3665200,
      "dDurationMs": 7440,
      "segs": [
        {
          "utf8": "going to look for something to maximize what quantity should we maximize so that"
        }
      ]
    },
    {
      "tStartMs": 3672640,
      "dDurationMs": 5479,
      "segs": [
        {
          "utf8": "our policy performs as best as possible such a thing is described as an"
        }
      ]
    },
    {
      "tStartMs": 3678119,
      "dDurationMs": 6281,
      "segs": [
        {
          "utf8": "objective function J which is written in a way that says it takes in the whole"
        }
      ]
    },
    {
      "tStartMs": 3684400,
      "dDurationMs": 5480,
      "segs": [
        {
          "utf8": "policy with all its Network parameters Theta the whole policy not just a"
        }
      ]
    },
    {
      "tStartMs": 3689880,
      "dDurationMs": 6080,
      "segs": [
        {
          "utf8": "specific state or action and outputs some sort of evaluation of how well it's"
        }
      ]
    },
    {
      "tStartMs": 3695960,
      "dDurationMs": 5920,
      "segs": [
        {
          "utf8": "doing so it's kind of like another type of evaluation but it's different from"
        }
      ]
    },
    {
      "tStartMs": 3701880,
      "dDurationMs": 5640,
      "segs": [
        {
          "utf8": "the value function such as Q in that Q for example takes a lot of effort to"
        }
      ]
    },
    {
      "tStartMs": 3707520,
      "dDurationMs": 4440,
      "segs": [
        {
          "utf8": "estimate and calculate and it actually serves the useful function of helping us"
        }
      ]
    },
    {
      "tStartMs": 3711960,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "differentiate between which states and actions are good and which stat actions"
        }
      ]
    },
    {
      "tStartMs": 3716520,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "are bad J on the other hand just evaluates how well the policy is doing"
        }
      ]
    },
    {
      "tStartMs": 3721880,
      "dDurationMs": 4760,
      "segs": [
        {
          "utf8": "as a whole so we could even say that our graph that tracks Total Rewards over"
        }
      ]
    },
    {
      "tStartMs": 3726640,
      "dDurationMs": 5000,
      "segs": [
        {
          "utf8": "time is an example of a j function calculating the value of J is not that"
        }
      ]
    },
    {
      "tStartMs": 3731640,
      "dDurationMs": 5719,
      "segs": [
        {
          "utf8": "hard and not that meaningful I mean we kind of just did with this reward graph"
        }
      ]
    },
    {
      "tStartMs": 3737359,
      "dDurationMs": 4601,
      "segs": [
        {
          "utf8": "and it doesn't really tell us anything about how to get better so the point of"
        }
      ]
    },
    {
      "tStartMs": 3741960,
      "dDurationMs": 6399,
      "segs": [
        {
          "utf8": "J is not really to calculate its value unlike the value functions of Q or V the"
        }
      ]
    },
    {
      "tStartMs": 3748359,
      "dDurationMs": 4841,
      "segs": [
        {
          "utf8": "point is that when you set it as the thing to maximize for this neural"
        }
      ]
    },
    {
      "tStartMs": 3753200,
      "dDurationMs": 5639,
      "segs": [
        {
          "utf8": "network through gradient Ascent you can end up deriving some pretty useful math"
        }
      ]
    },
    {
      "tStartMs": 3758839,
      "dDurationMs": 4480,
      "segs": [
        {
          "utf8": "that helps improve the policy function so here's where we have to figure out"
        }
      ]
    },
    {
      "tStartMs": 3763319,
      "dDurationMs": 6321,
      "segs": [
        {
          "utf8": "how to actually Define j in order to derive that math J is often defined as"
        }
      ]
    },
    {
      "tStartMs": 3769640,
      "dDurationMs": 4719,
      "segs": [
        {
          "utf8": "different things in different sources on the internet this one frames it as"
        }
      ]
    },
    {
      "tStartMs": 3774359,
      "dDurationMs": 3680,
      "segs": [
        {
          "utf8": "what's the expected total total reward that you'll get in every trajectory"
        }
      ]
    },
    {
      "tStartMs": 3778039,
      "dDurationMs": 4881,
      "segs": [
        {
          "utf8": "generated by the policy this one is what's the state value V of an initial"
        }
      ]
    },
    {
      "tStartMs": 3782920,
      "dDurationMs": 3960,
      "segs": [
        {
          "utf8": "States in an episode with a start and end this one is the average State value"
        }
      ]
    },
    {
      "tStartMs": 3786880,
      "dDurationMs": 5120,
      "segs": [
        {
          "utf8": "V across all states and this one is the average reward per time step the point"
        }
      ]
    },
    {
      "tStartMs": 3792000,
      "dDurationMs": 4359,
      "segs": [
        {
          "utf8": "is it doesn't really matter they're basically all slightly different ways of"
        }
      ]
    },
    {
      "tStartMs": 3796359,
      "dDurationMs": 3920,
      "segs": [
        {
          "utf8": "saying the same thing which is how much reward are you going to get on average"
        }
      ]
    },
    {
      "tStartMs": 3800279,
      "dDurationMs": 6441,
      "segs": [
        {
          "utf8": "the definition of j isn't that important what is important is that with all these"
        }
      ]
    },
    {
      "tStartMs": 3806720,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "different definitions if you differentiate it with respect to the"
        }
      ]
    },
    {
      "tStartMs": 3811440,
      "dDurationMs": 4159,
      "segs": [
        {
          "utf8": "parameters Theta with a whole bunch of complicated math involving something"
        }
      ]
    },
    {
      "tStartMs": 3815599,
      "dDurationMs": 5200,
      "segs": [
        {
          "utf8": "called the policy gradients theorem you end up with something that is generally"
        }
      ]
    },
    {
      "tStartMs": 3820799,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "in the form of this now that looks really complicated so let's break down"
        }
      ]
    },
    {
      "tStartMs": 3825599,
      "dDurationMs": 3881,
      "segs": [
        {
          "utf8": "this equation right here first of all this thing right here is the gradients"
        }
      ]
    },
    {
      "tStartMs": 3829480,
      "dDurationMs": 7359,
      "segs": [
        {
          "utf8": "of J with respect to Theta which just means how to change the parameters Theta"
        }
      ]
    },
    {
      "tStartMs": 3836839,
      "dDurationMs": 5881,
      "segs": [
        {
          "utf8": "in order to increase the performance as measured by J and then this thing right"
        }
      ]
    },
    {
      "tStartMs": 3842720,
      "dDurationMs": 5559,
      "segs": [
        {
          "utf8": "here is you take Pi of a given s which is How likely you are to take a certain"
        }
      ]
    },
    {
      "tStartMs": 3848279,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "action and then you log it and then you take the gradient of that with respect"
        }
      ]
    },
    {
      "tStartMs": 3853559,
      "dDurationMs": 4081,
      "segs": [
        {
          "utf8": "to parameters Theta that sounds complicated but basically it's saying"
        }
      ]
    },
    {
      "tStartMs": 3857640,
      "dDurationMs": 5919,
      "segs": [
        {
          "utf8": "how to increase the probability of an action a this thing right here is some"
        }
      ]
    },
    {
      "tStartMs": 3863559,
      "dDurationMs": 4000,
      "segs": [
        {
          "utf8": "sort of quantity there's most multiple definitions for this quantity which"
        }
      ]
    },
    {
      "tStartMs": 3867559,
      "dDurationMs": 4961,
      "segs": [
        {
          "utf8": "we'll get to later on but it's some sort of quantity saying how good is that"
        }
      ]
    },
    {
      "tStartMs": 3872520,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "action a and then finally there's this expected value and sum across all the"
        }
      ]
    },
    {
      "tStartMs": 3877480,
      "dDurationMs": 3879,
      "segs": [
        {
          "utf8": "time steps which is basically to say you're averaging out across all these"
        }
      ]
    },
    {
      "tStartMs": 3881359,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "states and actions so basically it's saying in order to increase performance"
        }
      ]
    },
    {
      "tStartMs": 3887359,
      "dDurationMs": 7281,
      "segs": [
        {
          "utf8": "increase the probability of all actions according to how good each action is"
        }
      ]
    },
    {
      "tStartMs": 3894640,
      "dDurationMs": 3719,
      "segs": [
        {
          "utf8": "even though the math looks very complicated it's basically doing what we"
        }
      ]
    },
    {
      "tStartMs": 3898359,
      "dDurationMs": 4641,
      "segs": [
        {
          "utf8": "said at the start push up the probabilities for actions that did good"
        }
      ]
    },
    {
      "tStartMs": 3903000,
      "dDurationMs": 4599,
      "segs": [
        {
          "utf8": "and push down the probabilities for actions that did poorly so let's first"
        }
      ]
    },
    {
      "tStartMs": 3907599,
      "dDurationMs": 5401,
      "segs": [
        {
          "utf8": "look at this quantity that says how good each action is as we've said before we"
        }
      ]
    },
    {
      "tStartMs": 3913000,
      "dDurationMs": 5359,
      "segs": [
        {
          "utf8": "could just use GT which is the total return that occurred after that action"
        }
      ]
    },
    {
      "tStartMs": 3918359,
      "dDurationMs": 3801,
      "segs": [
        {
          "utf8": "but of course this suffers from the problem that different states might have"
        }
      ]
    },
    {
      "tStartMs": 3922160,
      "dDurationMs": 4080,
      "segs": [
        {
          "utf8": "higher or lower rewards on average so it's inconsistent"
        }
      ]
    },
    {
      "tStartMs": 3926240,
      "dDurationMs": 5400,
      "segs": [
        {
          "utf8": "and has what we call High variance we want to figure out ways to reduce this"
        }
      ]
    },
    {
      "tStartMs": 3931640,
      "dDurationMs": 4919,
      "segs": [
        {
          "utf8": "variance of course then we can do what we proposed which is to use the value"
        }
      ]
    },
    {
      "tStartMs": 3936559,
      "dDurationMs": 6280,
      "segs": [
        {
          "utf8": "function V as a Baseline and make it fair for all the different states but"
        }
      ]
    },
    {
      "tStartMs": 3942839,
      "dDurationMs": 4681,
      "segs": [
        {
          "utf8": "then this GT still suffers from the problem that you have to wait for a"
        }
      ]
    },
    {
      "tStartMs": 3947520,
      "dDurationMs": 5160,
      "segs": [
        {
          "utf8": "whole episode to end and you can't use temporal difference so you could also"
        }
      ]
    },
    {
      "tStartMs": 3952680,
      "dDurationMs": 4679,
      "segs": [
        {
          "utf8": "rewrite these two but replace it with the action value function Q so it could"
        }
      ]
    },
    {
      "tStartMs": 3957359,
      "dDurationMs": 6041,
      "segs": [
        {
          "utf8": "be either Q or Q minus V and now we're fully using temporal difference but then"
        }
      ]
    },
    {
      "tStartMs": 3963400,
      "dDurationMs": 6439,
      "segs": [
        {
          "utf8": "with this Q minus V it's too much effort to have to use two different networks"
        }
      ]
    },
    {
      "tStartMs": 3969839,
      "dDurationMs": 6121,
      "segs": [
        {
          "utf8": "one for Q and one for V so you can use this equivalence form which rewrites Q"
        }
      ]
    },
    {
      "tStartMs": 3975960,
      "dDurationMs": 4639,
      "segs": [
        {
          "utf8": "as the immediate rewards plus V for the next state multiplied by gamma so now we"
        }
      ]
    },
    {
      "tStartMs": 3980599,
      "dDurationMs": 5081,
      "segs": [
        {
          "utf8": "only need V and then we have the most advanced and Powerful way which is"
        }
      ]
    },
    {
      "tStartMs": 3985680,
      "dDurationMs": 3960,
      "segs": [
        {
          "utf8": "method called generalized Advantage estimate where the math is a bit"
        }
      ]
    },
    {
      "tStartMs": 3989640,
      "dDurationMs": 4360,
      "segs": [
        {
          "utf8": "complicated so I won't write it out the point is there's quite a few different"
        }
      ]
    },
    {
      "tStartMs": 3994000,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "ways of calculating and they generally aim to go from evaluating how good each"
        }
      ]
    },
    {
      "tStartMs": 3999440,
      "dDurationMs": 4520,
      "segs": [
        {
          "utf8": "action is on its own which is what we did with the value based methods like"
        }
      ]
    },
    {
      "tStartMs": 4003960,
      "dDurationMs": 5920,
      "segs": [
        {
          "utf8": "deep Q learning to evaluating how good each action is relative to the other"
        }
      ]
    },
    {
      "tStartMs": 4009880,
      "dDurationMs": 6520,
      "segs": [
        {
          "utf8": "actions in the same state also known as its advantage and this is with the goal"
        }
      ]
    },
    {
      "tStartMs": 4016400,
      "dDurationMs": 5439,
      "segs": [
        {
          "utf8": "of reducing variance notice how the variance was not really an issue with"
        }
      ]
    },
    {
      "tStartMs": 4021839,
      "dDurationMs": 4681,
      "segs": [
        {
          "utf8": "these value based methods like deep Q Network because you're just picking the"
        }
      ]
    },
    {
      "tStartMs": 4026520,
      "dDurationMs": 4799,
      "segs": [
        {
          "utf8": "best action anyways but now it's important to consider in order to adjust"
        }
      ]
    },
    {
      "tStartMs": 4031319,
      "dDurationMs": 4201,
      "segs": [
        {
          "utf8": "these action probabilities proportionally so now that we've looked"
        }
      ]
    },
    {
      "tStartMs": 4035520,
      "dDurationMs": 5240,
      "segs": [
        {
          "utf8": "at how to calculate how good each action is let's look at the other part of the"
        }
      ]
    },
    {
      "tStartMs": 4040760,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "equation which is how to increase the probability of said action here we have"
        }
      ]
    },
    {
      "tStartMs": 4046760,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "two main cases one where it's a discrete probability distribution and one where"
        }
      ]
    },
    {
      "tStartMs": 4052200,
      "dDurationMs": 5119,
      "segs": [
        {
          "utf8": "it's a continuous one and we'll say that the continuous one is a normal or"
        }
      ]
    },
    {
      "tStartMs": 4057319,
      "dDurationMs": 5921,
      "segs": [
        {
          "utf8": "gaussian distribution so it has a mean and standard deviation the discrete one"
        }
      ]
    },
    {
      "tStartMs": 4063240,
      "dDurationMs": 4599,
      "segs": [
        {
          "utf8": "is simple whichever action it is you just push the probability for that one"
        }
      ]
    },
    {
      "tStartMs": 4067839,
      "dDurationMs": 6081,
      "segs": [
        {
          "utf8": "up or down and if you're using a softmax function the other probabilities will"
        }
      ]
    },
    {
      "tStartMs": 4073920,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "adjust to accommodate it with the continuous one the actions will now be a"
        }
      ]
    },
    {
      "tStartMs": 4079920,
      "dDurationMs": 5639,
      "segs": [
        {
          "utf8": "continuous range of numbers and what you're doing instead is whatever number"
        }
      ]
    },
    {
      "tStartMs": 4085559,
      "dDurationMs": 5961,
      "segs": [
        {
          "utf8": "the action is you're dragging the mean towards that number proportional to how"
        }
      ]
    },
    {
      "tStartMs": 4091520,
      "dDurationMs": 5400,
      "segs": [
        {
          "utf8": "far away it is and with the standard deviation then you can just increase it"
        }
      ]
    },
    {
      "tStartMs": 4096920,
      "dDurationMs": 4759,
      "segs": [
        {
          "utf8": "if the action is far away from the mean and decrease it if the action is close"
        }
      ]
    },
    {
      "tStartMs": 4101679,
      "dDurationMs": 5000,
      "segs": [
        {
          "utf8": "by each action is weighted according to how good it"
        }
      ]
    },
    {
      "tStartMs": 4106679,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "is so now we've covered our two main components of optimizing this objective"
        }
      ]
    },
    {
      "tStartMs": 4112679,
      "dDurationMs": 4680,
      "segs": [
        {
          "utf8": "function one is adjusting the action probabilities which is done by the"
        }
      ]
    },
    {
      "tStartMs": 4117359,
      "dDurationMs": 5000,
      "segs": [
        {
          "utf8": "policy function and the other one is evaluating how good each action is to"
        }
      ]
    },
    {
      "tStartMs": 4122359,
      "dDurationMs": 5561,
      "segs": [
        {
          "utf8": "inform that adjustment and this is done with at least one value function"
        }
      ]
    },
    {
      "tStartMs": 4127920,
      "dDurationMs": 5839,
      "segs": [
        {
          "utf8": "sometimes more if this evaluation process doesn't use this return value"
        }
      ]
    },
    {
      "tStartMs": 4133759,
      "dDurationMs": 4881,
      "segs": [
        {
          "utf8": "that you have to wait till the the end of an episode to figure out and instead"
        }
      ]
    },
    {
      "tStartMs": 4138640,
      "dDurationMs": 5800,
      "segs": [
        {
          "utf8": "fully uses temporal difference then usually it's called an actor critic"
        }
      ]
    },
    {
      "tStartMs": 4144440,
      "dDurationMs": 6680,
      "segs": [
        {
          "utf8": "method essentially the actor which is the policy function acts so it picks"
        }
      ]
    },
    {
      "tStartMs": 4151120,
      "dDurationMs": 5960,
      "segs": [
        {
          "utf8": "action and tries to improve and the critic which is the evaluation with"
        }
      ]
    },
    {
      "tStartMs": 4157080,
      "dDurationMs": 5599,
      "segs": [
        {
          "utf8": "value functions critiques the actor in order to help it improve now I don't"
        }
      ]
    },
    {
      "tStartMs": 4162679,
      "dDurationMs": 5361,
      "segs": [
        {
          "utf8": "think it's too important to know the fine detail of all this math as long as"
        }
      ]
    },
    {
      "tStartMs": 4168040,
      "dDurationMs": 5000,
      "segs": [
        {
          "utf8": "you intuitively understand what it's doing which is push probabilities for"
        }
      ]
    },
    {
      "tStartMs": 4173040,
      "dDurationMs": 4840,
      "segs": [
        {
          "utf8": "good actions up and probabilities for bad actions down using a range of"
        }
      ]
    },
    {
      "tStartMs": 4177880,
      "dDurationMs": 5000,
      "segs": [
        {
          "utf8": "different methods to evaluate how good or bad each action was in order to"
        }
      ]
    },
    {
      "tStartMs": 4182880,
      "dDurationMs": 5600,
      "segs": [
        {
          "utf8": "improve overall performance with the mathematical equations specifically with"
        }
      ]
    },
    {
      "tStartMs": 4188480,
      "dDurationMs": 5719,
      "segs": [
        {
          "utf8": "these typical objective functions which result in a gradients of this form some"
        }
      ]
    },
    {
      "tStartMs": 4194199,
      "dDurationMs": 6881,
      "segs": [
        {
          "utf8": "of the more advanced r l algorithms don't even use this for example trpo and"
        }
      ]
    },
    {
      "tStartMs": 4201080,
      "dDurationMs": 6200,
      "segs": [
        {
          "utf8": "poo are two algorithms that use what's known as surrogate objective functions"
        }
      ]
    },
    {
      "tStartMs": 4207280,
      "dDurationMs": 4959,
      "segs": [
        {
          "utf8": "which are like the objective function J but formulated differently it looks"
        }
      ]
    },
    {
      "tStartMs": 4212239,
      "dDurationMs": 5281,
      "segs": [
        {
          "utf8": "really complicated and when the neural network tries to optimize that it often"
        }
      ]
    },
    {
      "tStartMs": 4217520,
      "dDurationMs": 4679,
      "segs": [
        {
          "utf8": "performs better than when it tries to optimize this stuff but it still follows"
        }
      ]
    },
    {
      "tStartMs": 4222199,
      "dDurationMs": 4201,
      "segs": [
        {
          "utf8": "the general structure of an actor and a Critic"
        }
      ]
    },
    {
      "tStartMs": 4226400,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "so here is lunar lander with a policy gradients method I've downloaded this"
        }
      ]
    },
    {
      "tStartMs": 4232400,
      "dDurationMs": 4440,
      "segs": [
        {
          "utf8": "trained model from the internet and it was trained with the algorithm proximal"
        }
      ]
    },
    {
      "tStartMs": 4236840,
      "dDurationMs": 5399,
      "segs": [
        {
          "utf8": "policy optimization poo so a bit different from the classical objective"
        }
      ]
    },
    {
      "tStartMs": 4242239,
      "dDurationMs": 3841,
      "segs": [
        {
          "utf8": "function gradients that we looked at but essentially you can see that instead of"
        }
      ]
    },
    {
      "tStartMs": 4246080,
      "dDurationMs": 4639,
      "segs": [
        {
          "utf8": "having one network to calculate evaluations for all actions within a"
        }
      ]
    },
    {
      "tStartMs": 4250719,
      "dDurationMs": 4841,
      "segs": [
        {
          "utf8": "state we have two networks one to calculate probabilities for all action"
        }
      ]
    },
    {
      "tStartMs": 4255560,
      "dDurationMs": 5400,
      "segs": [
        {
          "utf8": "within a state and one to calculate a single evaluation for the whole state in"
        }
      ]
    },
    {
      "tStartMs": 4260960,
      "dDurationMs": 4680,
      "segs": [
        {
          "utf8": "order to help with adjusting those probabilities in this case the state"
        }
      ]
    },
    {
      "tStartMs": 4265640,
      "dDurationMs": 4880,
      "segs": [
        {
          "utf8": "value function V within the probabilities the red one represents"
        }
      ]
    },
    {
      "tStartMs": 4270520,
      "dDurationMs": 3920,
      "segs": [
        {
          "utf8": "which action it actually ended up picking so you can see that there is"
        }
      ]
    },
    {
      "tStartMs": 4274440,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "inbuilt Randomness here unlike with value based methods like deep Q networks"
        }
      ]
    },
    {
      "tStartMs": 4279480,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "where you pick the best option but artificially add in Randomness with"
        }
      ]
    },
    {
      "tStartMs": 4283640,
      "dDurationMs": 5960,
      "segs": [
        {
          "utf8": "Epsilon greedy the next example is called bipedal Walker and has continuous"
        }
      ]
    },
    {
      "tStartMs": 4289600,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "actions instead of discrete actions this model was trained using a method called"
        }
      ]
    },
    {
      "tStartMs": 4294400,
      "dDurationMs": 5720,
      "segs": [
        {
          "utf8": "Soft Axor critic sa and you can see that not only have we gone from multiple"
        }
      ]
    },
    {
      "tStartMs": 4300120,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "discrete options to one continuous distribution we also now have multiple"
        }
      ]
    },
    {
      "tStartMs": 4305400,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "continuous distributions for all of the joints of this creature the actual"
        }
      ]
    },
    {
      "tStartMs": 4310120,
      "dDurationMs": 3400,
      "segs": [
        {
          "utf8": "actions that it ended up taking out of these probability distributions are"
        }
      ]
    },
    {
      "tStartMs": 4313520,
      "dDurationMs": 4719,
      "segs": [
        {
          "utf8": "written here on the right this time the advantage is calculated using the"
        }
      ]
    },
    {
      "tStartMs": 4318239,
      "dDurationMs": 4801,
      "segs": [
        {
          "utf8": "average action value function Q for all the actions instead of the state value"
        }
      ]
    },
    {
      "tStartMs": 4323040,
      "dDurationMs": 2560,
      "segs": [
        {
          "utf8": "function"
        }
      ]
    },
    {
      "tStartMs": 4328100,
      "dDurationMs": 3089,
      "segs": [
        {
          "utf8": "[Music]"
        }
      ]
    },
    {
      "tStartMs": 4331560,
      "dDurationMs": 3760,
      "segs": [
        {
          "utf8": "V let's talk about one of the most interesting things about reinforcement"
        }
      ]
    },
    {
      "tStartMs": 4335320,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "learning which is that throughout its development people have often found"
        }
      ]
    },
    {
      "tStartMs": 4339480,
      "dDurationMs": 5880,
      "segs": [
        {
          "utf8": "links to neuroscience and how the brain works here I'll cover two of the most"
        }
      ]
    },
    {
      "tStartMs": 4345360,
      "dDurationMs": 3839,
      "segs": [
        {
          "utf8": "well-known and well accepted similarities between reinforcement"
        }
      ]
    },
    {
      "tStartMs": 4349199,
      "dDurationMs": 5361,
      "segs": [
        {
          "utf8": "learning and the Brain the first is with temporal difference and dopamine and the"
        }
      ]
    },
    {
      "tStartMs": 4354560,
      "dDurationMs": 5800,
      "segs": [
        {
          "utf8": "second is with acor critic and the dorsal lateral and ventral strim so"
        }
      ]
    },
    {
      "tStartMs": 4360360,
      "dDurationMs": 5359,
      "segs": [
        {
          "utf8": "let's first look at temporal difference and dopamine specifically the rewards"
        }
      ]
    },
    {
      "tStartMs": 4365719,
      "dDurationMs": 6161,
      "segs": [
        {
          "utf8": "prediction error hypothesis of dopamine in the 1990s a scientist named wolam"
        }
      ]
    },
    {
      "tStartMs": 4371880,
      "dDurationMs": 4520,
      "segs": [
        {
          "utf8": "Schultz and his colleagues performed a well-known series of experiments on"
        }
      ]
    },
    {
      "tStartMs": 4376400,
      "dDurationMs": 5000,
      "segs": [
        {
          "utf8": "monkeys where monkeys were trained to press a lever after seeing a light slide"
        }
      ]
    },
    {
      "tStartMs": 4381400,
      "dDurationMs": 5560,
      "segs": [
        {
          "utf8": "up as a trigger queue and as a reward they would receive some apple juice in"
        }
      ]
    },
    {
      "tStartMs": 4386960,
      "dDurationMs": 5320,
      "segs": [
        {
          "utf8": "their mouth what happened was at first before the monkeys got good at this task"
        }
      ]
    },
    {
      "tStartMs": 4392280,
      "dDurationMs": 4280,
      "segs": [
        {
          "utf8": "whenever the monkeys got apple juice their dopamine neurons would get a sharp"
        }
      ]
    },
    {
      "tStartMs": 4396560,
      "dDurationMs": 5880,
      "segs": [
        {
          "utf8": "spike in activity which makes it seem like dopamine signals reward however"
        }
      ]
    },
    {
      "tStartMs": 4402440,
      "dDurationMs": 3360,
      "segs": [
        {
          "utf8": "after the monkeys got good at the task they learned that that the lights"
        }
      ]
    },
    {
      "tStartMs": 4405800,
      "dDurationMs": 4879,
      "segs": [
        {
          "utf8": "lighting up signaled that a drop of apple juice was about to come and their"
        }
      ]
    },
    {
      "tStartMs": 4410679,
      "dDurationMs": 5321,
      "segs": [
        {
          "utf8": "dopamine no longer spiked in response to the actual juice itself but rather to"
        }
      ]
    },
    {
      "tStartMs": 4416000,
      "dDurationMs": 5400,
      "segs": [
        {
          "utf8": "the light that predicted it afterwards in a different experiment there was not"
        }
      ]
    },
    {
      "tStartMs": 4421400,
      "dDurationMs": 4880,
      "segs": [
        {
          "utf8": "one but two levers and there would be a light sliding up above the correct lever"
        }
      ]
    },
    {
      "tStartMs": 4426280,
      "dDurationMs": 5800,
      "segs": [
        {
          "utf8": "first as an instruction cue after which there would be the trigger queue telling"
        }
      ]
    },
    {
      "tStartMs": 4432080,
      "dDurationMs": 4040,
      "segs": [
        {
          "utf8": "the monkey to press the lever after which the monkey would receive the apple"
        }
      ]
    },
    {
      "tStartMs": 4436120,
      "dDurationMs": 5039,
      "segs": [
        {
          "utf8": "juice now as the monkeys got good at this task the dopamine neurons stopped"
        }
      ]
    },
    {
      "tStartMs": 4441159,
      "dDurationMs": 3881,
      "segs": [
        {
          "utf8": "responding to the trigger que and started responding to the instruction"
        }
      ]
    },
    {
      "tStartMs": 4445040,
      "dDurationMs": 5720,
      "segs": [
        {
          "utf8": "cue so this continual shifting forward of the dopamine Spike suggests that it"
        }
      ]
    },
    {
      "tStartMs": 4450760,
      "dDurationMs": 5600,
      "segs": [
        {
          "utf8": "signifies not reward but rather the earliest sign that predicts there will"
        }
      ]
    },
    {
      "tStartMs": 4456360,
      "dDurationMs": 5480,
      "segs": [
        {
          "utf8": "be a reward so how does this relate to temporal difference well think about our"
        }
      ]
    },
    {
      "tStartMs": 4461840,
      "dDurationMs": 5359,
      "segs": [
        {
          "utf8": "value functions q and V let's say specifically V in this context which is"
        }
      ]
    },
    {
      "tStartMs": 4467199,
      "dDurationMs": 4401,
      "segs": [
        {
          "utf8": "supposed to predict how much reward an agents will end up getting the monkeys"
        }
      ]
    },
    {
      "tStartMs": 4471600,
      "dDurationMs": 6559,
      "segs": [
        {
          "utf8": "start off by expecting zero reward so their V function starts off at zero but"
        }
      ]
    },
    {
      "tStartMs": 4478159,
      "dDurationMs": 4641,
      "segs": [
        {
          "utf8": "at whatever point in time the monkey realizes it's going to get a reward the"
        }
      ]
    },
    {
      "tStartMs": 4482800,
      "dDurationMs": 7000,
      "segs": [
        {
          "utf8": "V function changes to being positive so conceptually you can think of this"
        }
      ]
    },
    {
      "tStartMs": 4489800,
      "dDurationMs": 5960,
      "segs": [
        {
          "utf8": "dopamine Spike as a surprise or a shift in expectation"
        }
      ]
    },
    {
      "tStartMs": 4495760,
      "dDurationMs": 4760,
      "segs": [
        {
          "utf8": "or even a prediction error because if the V function was supposed to predict"
        }
      ]
    },
    {
      "tStartMs": 4500520,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "future reward then you could say that when the new information arrived and it"
        }
      ]
    },
    {
      "tStartMs": 4505560,
      "dDurationMs": 6119,
      "segs": [
        {
          "utf8": "changed there was a realization that the old prediction was wrong that there was"
        }
      ]
    },
    {
      "tStartMs": 4511679,
      "dDurationMs": 5161,
      "segs": [
        {
          "utf8": "an error so that's how you can think of it conceptually but mathematically it's"
        }
      ]
    },
    {
      "tStartMs": 4516840,
      "dDurationMs": 6520,
      "segs": [
        {
          "utf8": "a concept called the temporal difference error or TD error remember how when you"
        }
      ]
    },
    {
      "tStartMs": 4523360,
      "dDurationMs": 5000,
      "segs": [
        {
          "utf8": "are learning your value functions you do so with these temporal difference"
        }
      ]
    },
    {
      "tStartMs": 4528360,
      "dDurationMs": 6799,
      "segs": [
        {
          "utf8": "updates where you're moving your expectation to some sort of Target well"
        }
      ]
    },
    {
      "tStartMs": 4535159,
      "dDurationMs": 4761,
      "segs": [
        {
          "utf8": "in order to do that update of course you'd have to first figure out what the"
        }
      ]
    },
    {
      "tStartMs": 4539920,
      "dDurationMs": 5120,
      "segs": [
        {
          "utf8": "difference is between that Target and what your expectation is currently the"
        }
      ]
    },
    {
      "tStartMs": 4545040,
      "dDurationMs": 5760,
      "segs": [
        {
          "utf8": "difference between what you originally expected to get versus what you actually"
        }
      ]
    },
    {
      "tStartMs": 4550800,
      "dDurationMs": 5359,
      "segs": [
        {
          "utf8": "got plus what you now expect to get with the new information"
        }
      ]
    },
    {
      "tStartMs": 4556159,
      "dDurationMs": 5201,
      "segs": [
        {
          "utf8": "this is the TD error of course you could write out the TD error for the Q"
        }
      ]
    },
    {
      "tStartMs": 4561360,
      "dDurationMs": 4120,
      "segs": [
        {
          "utf8": "function as well and in fact you could write multiple different forms of it"
        }
      ]
    },
    {
      "tStartMs": 4565480,
      "dDurationMs": 4320,
      "segs": [
        {
          "utf8": "since there's different ways of learning the Q function but we're focusing on the"
        }
      ]
    },
    {
      "tStartMs": 4569800,
      "dDurationMs": 5919,
      "segs": [
        {
          "utf8": "V function here because it's simpler and the TD error usually refers to the V"
        }
      ]
    },
    {
      "tStartMs": 4575719,
      "dDurationMs": 6881,
      "segs": [
        {
          "utf8": "function either way this TD error is the signal that guides learning now remember"
        }
      ]
    },
    {
      "tStartMs": 4582600,
      "dDurationMs": 4039,
      "segs": [
        {
          "utf8": "that through this learning temporal differ difference is essentially"
        }
      ]
    },
    {
      "tStartMs": 4586639,
      "dDurationMs": 4520,
      "segs": [
        {
          "utf8": "propagating knowledge of reward back through the time steps which is what"
        }
      ]
    },
    {
      "tStartMs": 4591159,
      "dDurationMs": 4841,
      "segs": [
        {
          "utf8": "enables this value function to predict and capture information from the future"
        }
      ]
    },
    {
      "tStartMs": 4596000,
      "dDurationMs": 5320,
      "segs": [
        {
          "utf8": "if there's a consistent cue like the trigger queue or instruction queue which"
        }
      ]
    },
    {
      "tStartMs": 4601320,
      "dDurationMs": 7319,
      "segs": [
        {
          "utf8": "you can think of as a state then the TD error the surprise of reward gets"
        }
      ]
    },
    {
      "tStartMs": 4608639,
      "dDurationMs": 5361,
      "segs": [
        {
          "utf8": "propagated back to there which is why this dopamine shift occurs until there's"
        }
      ]
    },
    {
      "tStartMs": 4614000,
      "dDurationMs": 5400,
      "segs": [
        {
          "utf8": "no Q before that which can predict it anymore at which point you can think of"
        }
      ]
    },
    {
      "tStartMs": 4619400,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "it as there not being a consistent earlier state so this learning cannot"
        }
      ]
    },
    {
      "tStartMs": 4625400,
      "dDurationMs": 4920,
      "segs": [
        {
          "utf8": "happen what's interesting is the dopamine also showed signs of being a"
        }
      ]
    },
    {
      "tStartMs": 4630320,
      "dDurationMs": 5160,
      "segs": [
        {
          "utf8": "negative shift in expectation when the monkeys were expecting apple juice but"
        }
      ]
    },
    {
      "tStartMs": 4635480,
      "dDurationMs": 3759,
      "segs": [
        {
          "utf8": "they accidentally hit the wrong lever so they didn't get it their dopamine"
        }
      ]
    },
    {
      "tStartMs": 4639239,
      "dDurationMs": 5161,
      "segs": [
        {
          "utf8": "neurons experienced a sharp drop in activity signaling a negative temporal"
        }
      ]
    },
    {
      "tStartMs": 4644400,
      "dDurationMs": 4920,
      "segs": [
        {
          "utf8": "difference error they're now returning from an expectation of positive reward"
        }
      ]
    },
    {
      "tStartMs": 4649320,
      "dDurationMs": 5319,
      "segs": [
        {
          "utf8": "to an expectation of zero reward so anyways that's the link between dopamine"
        }
      ]
    },
    {
      "tStartMs": 4654639,
      "dDurationMs": 5121,
      "segs": [
        {
          "utf8": "and temporal difference the reward prediction error hypothesis of dopamine"
        }
      ]
    },
    {
      "tStartMs": 4659760,
      "dDurationMs": 4959,
      "segs": [
        {
          "utf8": "extending upon that is how part of the brain exhibits similar Behavior to an"
        }
      ]
    },
    {
      "tStartMs": 4664719,
      "dDurationMs": 5081,
      "segs": [
        {
          "utf8": "actor critic structure this is now a more nuanced topic and I won't be able"
        }
      ]
    },
    {
      "tStartMs": 4669800,
      "dDurationMs": 4520,
      "segs": [
        {
          "utf8": "to refer to one single study that's as obvious as the monkey one but let's look"
        }
      ]
    },
    {
      "tStartMs": 4674320,
      "dDurationMs": 5120,
      "segs": [
        {
          "utf8": "at a diagram of a human brain as you may or may not know different parts of the"
        }
      ]
    },
    {
      "tStartMs": 4679440,
      "dDurationMs": 4199,
      "segs": [
        {
          "utf8": "brain are responsible for different things you've got areas responsible for"
        }
      ]
    },
    {
      "tStartMs": 4683639,
      "dDurationMs": 6080,
      "segs": [
        {
          "utf8": "sensory input motor output visual perception language and speech Etc this"
        }
      ]
    },
    {
      "tStartMs": 4689719,
      "dDurationMs": 4401,
      "segs": [
        {
          "utf8": "image shows the outside of the brain if we look inside the brain you'll see we"
        }
      ]
    },
    {
      "tStartMs": 4694120,
      "dDurationMs": 4760,
      "segs": [
        {
          "utf8": "have this green bit here called the striatum and that's responsible for"
        }
      ]
    },
    {
      "tStartMs": 4698880,
      "dDurationMs": 5960,
      "segs": [
        {
          "utf8": "according to Wikipedia motor and action planning decision making motivation"
        }
      ]
    },
    {
      "tStartMs": 4704840,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "reinforcement and reward perception sounds familiar to what reinforcement"
        }
      ]
    },
    {
      "tStartMs": 4709400,
      "dDurationMs": 4200,
      "segs": [
        {
          "utf8": "learning is doing right well it turns out there's been extensive evidence that"
        }
      ]
    },
    {
      "tStartMs": 4713600,
      "dDurationMs": 5079,
      "segs": [
        {
          "utf8": "two subparts of the striatum in particular behave similarly to an actor"
        }
      ]
    },
    {
      "tStartMs": 4718679,
      "dDurationMs": 5601,
      "segs": [
        {
          "utf8": "and a Critic the dorsal lateral striatum participates in action selection and"
        }
      ]
    },
    {
      "tStartMs": 4724280,
      "dDurationMs": 5720,
      "segs": [
        {
          "utf8": "execution like the actor and the ventral striatum participates in evaluation and"
        }
      ]
    },
    {
      "tStartMs": 4730000,
      "dDurationMs": 6360,
      "segs": [
        {
          "utf8": "prediction in order to guide the actor like the critic this 2008 article goes"
        }
      ]
    },
    {
      "tStartMs": 4736360,
      "dDurationMs": 4200,
      "segs": [
        {
          "utf8": "into detail on the topic and they've provided a neat little diagram here"
        }
      ]
    },
    {
      "tStartMs": 4740560,
      "dDurationMs": 3639,
      "segs": [
        {
          "utf8": "where on the left side are the components of our typical reinforcement"
        }
      ]
    },
    {
      "tStartMs": 4744199,
      "dDurationMs": 4881,
      "segs": [
        {
          "utf8": "learning framework and on the right side is how their functions map on two parts"
        }
      ]
    },
    {
      "tStartMs": 4749080,
      "dDurationMs": 5880,
      "segs": [
        {
          "utf8": "of the brain if you'll see here in a usual actor critic RL algorithm the TD"
        }
      ]
    },
    {
      "tStartMs": 4754960,
      "dDurationMs": 4679,
      "segs": [
        {
          "utf8": "error feeds into both the actor and the critic from our previous section on"
        }
      ]
    },
    {
      "tStartMs": 4759639,
      "dDurationMs": 5161,
      "segs": [
        {
          "utf8": "dopamine you might remember that the TD error is a signal that guides learning"
        }
      ]
    },
    {
      "tStartMs": 4764800,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "of a value function so it makes sense that it feeds into the critic but you"
        }
      ]
    },
    {
      "tStartMs": 4769840,
      "dDurationMs": 5640,
      "segs": [
        {
          "utf8": "might also remember that in the math of policy gradients one of the ways of"
        }
      ]
    },
    {
      "tStartMs": 4775480,
      "dDurationMs": 6480,
      "segs": [
        {
          "utf8": "calculating Advantage is also the TD error so it also informs the actor on"
        }
      ]
    },
    {
      "tStartMs": 4781960,
      "dDurationMs": 5880,
      "segs": [
        {
          "utf8": "how to improve and sure enough in the human brain the dopamine signals which"
        }
      ]
    },
    {
      "tStartMs": 4787840,
      "dDurationMs": 5000,
      "segs": [
        {
          "utf8": "are supposed to behave like the TDR feed into both the ventral striatum and the"
        }
      ]
    },
    {
      "tStartMs": 4792840,
      "dDurationMs": 4200,
      "segs": [
        {
          "utf8": "dorsal lateral striatum that's just one part of the evidence that supports this"
        }
      ]
    },
    {
      "tStartMs": 4797040,
      "dDurationMs": 3400,
      "segs": [
        {
          "utf8": "correlation which we are able to understand with the stuff that we just"
        }
      ]
    },
    {
      "tStartMs": 4800440,
      "dDurationMs": 3840,
      "segs": [
        {
          "utf8": "went through the other evidence involves a lot more Neuroscience knowledge which"
        }
      ]
    },
    {
      "tStartMs": 4804280,
      "dDurationMs": 3280,
      "segs": [
        {
          "utf8": "you might be able to see with all these fancy names of different parts of the"
        }
      ]
    },
    {
      "tStartMs": 4807560,
      "dDurationMs": 4520,
      "segs": [
        {
          "utf8": "brain so we won't go into that so the correlation between temporal difference"
        }
      ]
    },
    {
      "tStartMs": 4812080,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "and dopamine and actor critic and the dorsal lateral and vental stasm those"
        }
      ]
    },
    {
      "tStartMs": 4817120,
      "dDurationMs": 3160,
      "segs": [
        {
          "utf8": "are two of the main links that have been discovered between reinforcement"
        }
      ]
    },
    {
      "tStartMs": 4820280,
      "dDurationMs": 3879,
      "segs": [
        {
          "utf8": "learning and Neuroscience"
        }
      ]
    },
    {
      "tStartMs": 4826160,
      "dDurationMs": 3090,
      "segs": [
        {
          "utf8": "[Music]"
        }
      ]
    },
    {
      "tStartMs": 4830719,
      "dDurationMs": 3920,
      "segs": [
        {
          "utf8": "now I'll talk about some of the key Subs in reinforcement learning that I think"
        }
      ]
    },
    {
      "tStartMs": 4834639,
      "dDurationMs": 5281,
      "segs": [
        {
          "utf8": "are the most important for it to develop in the coming decades evidently we don't"
        }
      ]
    },
    {
      "tStartMs": 4839920,
      "dDurationMs": 4040,
      "segs": [
        {
          "utf8": "have robots in our homes that can fold our t-shirts yet and some of the main"
        }
      ]
    },
    {
      "tStartMs": 4843960,
      "dDurationMs": 6640,
      "segs": [
        {
          "utf8": "issues still holding back RL are covered in this famous article from 2018 titled"
        }
      ]
    },
    {
      "tStartMs": 4850600,
      "dDurationMs": 4440,
      "segs": [
        {
          "utf8": "deep reinforcement learning doesn't work yet many of its points are are still"
        }
      ]
    },
    {
      "tStartMs": 4855040,
      "dDurationMs": 5760,
      "segs": [
        {
          "utf8": "relevant today one of the main points was that RL can be really unreliable"
        }
      ]
    },
    {
      "tStartMs": 4860800,
      "dDurationMs": 4600,
      "segs": [
        {
          "utf8": "meaning that you could write some code to train a model and run it 10 times"
        }
      ]
    },
    {
      "tStartMs": 4865400,
      "dDurationMs": 4200,
      "segs": [
        {
          "utf8": "without changing anything other than the automatic random seed that all the"
        }
      ]
    },
    {
      "tStartMs": 4869600,
      "dDurationMs": 4680,
      "segs": [
        {
          "utf8": "networks are initialized by and then half the time it works and half the time"
        }
      ]
    },
    {
      "tStartMs": 4874280,
      "dDurationMs": 6320,
      "segs": [
        {
          "utf8": "it doesn't indeed when I was trying to train lunar lander with policy gradients"
        }
      ]
    },
    {
      "tStartMs": 4880600,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "specifically when I run the code multiple times without changing anything"
        }
      ]
    },
    {
      "tStartMs": 4885400,
      "dDurationMs": 3960,
      "segs": [
        {
          "utf8": "there were enormous differences between the performance of different runs this"
        }
      ]
    },
    {
      "tStartMs": 4889360,
      "dDurationMs": 3839,
      "segs": [
        {
          "utf8": "is part of why I ended up just downloading a model from the Internet or"
        }
      ]
    },
    {
      "tStartMs": 4893199,
      "dDurationMs": 4520,
      "segs": [
        {
          "utf8": "else I wouldn't finish this video on time reinforcement learning can also be"
        }
      ]
    },
    {
      "tStartMs": 4897719,
      "dDurationMs": 5361,
      "segs": [
        {
          "utf8": "really sample inefficient meaning it takes an insanely High number of time"
        }
      ]
    },
    {
      "tStartMs": 4903080,
      "dDurationMs": 5079,
      "segs": [
        {
          "utf8": "steps and episodes to train models in the article it talks about how rainbow"
        }
      ]
    },
    {
      "tStartMs": 4908159,
      "dDurationMs": 5361,
      "segs": [
        {
          "utf8": "dqn which was one of the best RL algorithms for learning Atari games was"
        }
      ]
    },
    {
      "tStartMs": 4913520,
      "dDurationMs": 5840,
      "segs": [
        {
          "utf8": "able to reach human level performance after 18 million frames now how long is"
        }
      ]
    },
    {
      "tStartMs": 4919360,
      "dDurationMs": 7960,
      "segs": [
        {
          "utf8": "18 million frames assuming 60 frames per second that's 83 hours so on tasks that"
        }
      ]
    },
    {
      "tStartMs": 4927320,
      "dDurationMs": 4480,
      "segs": [
        {
          "utf8": "might take a normal human a few minutes to learn reinforcement learning takes an"
        }
      ]
    },
    {
      "tStartMs": 4931800,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "almost laughable amount of time now that might be fine for video games where you"
        }
      ]
    },
    {
      "tStartMs": 4936840,
      "dDurationMs": 4319,
      "segs": [
        {
          "utf8": "have infinite simulation anyways but if you're trying to apply reinforcement"
        }
      ]
    },
    {
      "tStartMs": 4941159,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "learning to situations where you cannot be so example inefficient like the"
        }
      ]
    },
    {
      "tStartMs": 4946199,
      "dDurationMs": 5601,
      "segs": [
        {
          "utf8": "physical world then it's not practical to require 10 million 50 million or even"
        }
      ]
    },
    {
      "tStartMs": 4951800,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "100 million frames of training like some of these research papers show the"
        }
      ]
    },
    {
      "tStartMs": 4956520,
      "dDurationMs": 3040,
      "segs": [
        {
          "utf8": "article had many more points so you could go check that out if you're"
        }
      ]
    },
    {
      "tStartMs": 4959560,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "interested but the main point here is to talk about some key subfields of RL that"
        }
      ]
    },
    {
      "tStartMs": 4964520,
      "dDurationMs": 4199,
      "segs": [
        {
          "utf8": "are still in continuous development and will hopefully help resolve some of"
        }
      ]
    },
    {
      "tStartMs": 4968719,
      "dDurationMs": 4641,
      "segs": [
        {
          "utf8": "these problems I'll specifically focus on two subfields model-based"
        }
      ]
    },
    {
      "tStartMs": 4973360,
      "dDurationMs": 5279,
      "segs": [
        {
          "utf8": "reinforcement learning and imitation learning SL inverse reinforcement"
        }
      ]
    },
    {
      "tStartMs": 4978639,
      "dDurationMs": 5321,
      "segs": [
        {
          "utf8": "learning the first one is modelbased reinforcement learning this refers to"
        }
      ]
    },
    {
      "tStartMs": 4983960,
      "dDurationMs": 4880,
      "segs": [
        {
          "utf8": "that world model function that we talked about the one that we assumed we do not"
        }
      ]
    },
    {
      "tStartMs": 4988840,
      "dDurationMs": 4120,
      "segs": [
        {
          "utf8": "have access to throughout the whole video it's a very complex and difficult"
        }
      ]
    },
    {
      "tStartMs": 4992960,
      "dDurationMs": 4880,
      "segs": [
        {
          "utf8": "topic that takes way too much time to cover in this video but essentially a"
        }
      ]
    },
    {
      "tStartMs": 4997840,
      "dDurationMs": 4040,
      "segs": [
        {
          "utf8": "world model can help you predict the next state and reward if you take a"
        }
      ]
    },
    {
      "tStartMs": 5001880,
      "dDurationMs": 4759,
      "segs": [
        {
          "utf8": "certain action in a certain State meaning that it encodes some sort of"
        }
      ]
    },
    {
      "tStartMs": 5006639,
      "dDurationMs": 5121,
      "segs": [
        {
          "utf8": "understanding of how the world Works which allows you to imagine and simulate"
        }
      ]
    },
    {
      "tStartMs": 5011760,
      "dDurationMs": 3840,
      "segs": [
        {
          "utf8": "how things will turn out model-free reinforcement learning which we have"
        }
      ]
    },
    {
      "tStartMs": 5015600,
      "dDurationMs": 5800,
      "segs": [
        {
          "utf8": "been covering up until now I think is not a very practical way of learning for"
        }
      ]
    },
    {
      "tStartMs": 5021400,
      "dDurationMs": 5000,
      "segs": [
        {
          "utf8": "example let's say I'm doing a task where if this tennis ball touches the table I"
        }
      ]
    },
    {
      "tStartMs": 5026400,
      "dDurationMs": 4520,
      "segs": [
        {
          "utf8": "lose points without a world model without any understanding of the"
        }
      ]
    },
    {
      "tStartMs": 5030920,
      "dDurationMs": 4759,
      "segs": [
        {
          "utf8": "environment it's basically like hm I wonder what happen happens if I let go"
        }
      ]
    },
    {
      "tStartMs": 5035679,
      "dDurationMs": 6040,
      "segs": [
        {
          "utf8": "of the ball here oh I lose points well what about if I let go of the ball here"
        }
      ]
    },
    {
      "tStartMs": 5041719,
      "dDurationMs": 5641,
      "segs": [
        {
          "utf8": "okay I also lose points well what about here oh okay I also lose points and then"
        }
      ]
    },
    {
      "tStartMs": 5047360,
      "dDurationMs": 6560,
      "segs": [
        {
          "utf8": "maybe after 300 trials I'll figure out that if I let go of the ball above this"
        }
      ]
    },
    {
      "tStartMs": 5053920,
      "dDurationMs": 5080,
      "segs": [
        {
          "utf8": "half of the table I'll lose points and then I'll need another 300 trials to"
        }
      ]
    },
    {
      "tStartMs": 5059000,
      "dDurationMs": 4760,
      "segs": [
        {
          "utf8": "figure out what happens if it's this half of the table now as a human if you"
        }
      ]
    },
    {
      "tStartMs": 5063760,
      "dDurationMs": 4240,
      "segs": [
        {
          "utf8": "learn like this that without basic understanding of the environment with"
        }
      ]
    },
    {
      "tStartMs": 5068000,
      "dDurationMs": 6159,
      "segs": [
        {
          "utf8": "stuff like contact between objects inertia and gravity you look stupid and"
        }
      ]
    },
    {
      "tStartMs": 5074159,
      "dDurationMs": 3921,
      "segs": [
        {
          "utf8": "if you think about it that makes it all the more impressive that reinforcement"
        }
      ]
    },
    {
      "tStartMs": 5078080,
      "dDurationMs": 4880,
      "segs": [
        {
          "utf8": "learning is able to do all these tasks without a world model without any"
        }
      ]
    },
    {
      "tStartMs": 5082960,
      "dDurationMs": 3759,
      "segs": [
        {
          "utf8": "understanding of the environment but that's also why it takes 10 million 50"
        }
      ]
    },
    {
      "tStartMs": 5086719,
      "dDurationMs": 5561,
      "segs": [
        {
          "utf8": "million 100 million frames the next time you do any everyday task take notice"
        }
      ]
    },
    {
      "tStartMs": 5092280,
      "dDurationMs": 4280,
      "segs": [
        {
          "utf8": "that a world model is essential to understanding what's going to happen to"
        }
      ]
    },
    {
      "tStartMs": 5096560,
      "dDurationMs": 3720,
      "segs": [
        {
          "utf8": "a t-shirt when you fold it which way you're going to fall when you lose your"
        }
      ]
    },
    {
      "tStartMs": 5100280,
      "dDurationMs": 5080,
      "segs": [
        {
          "utf8": "balance how the water flows in a cup when you fill it up Etc so modelbased"
        }
      ]
    },
    {
      "tStartMs": 5105360,
      "dDurationMs": 6160,
      "segs": [
        {
          "utf8": "reinforcement learning in order to obtain a world model there's two methods"
        }
      ]
    },
    {
      "tStartMs": 5111520,
      "dDurationMs": 4760,
      "segs": [
        {
          "utf8": "a world model can either be handcrafted and given to the agent like if we"
        }
      ]
    },
    {
      "tStartMs": 5116280,
      "dDurationMs": 3760,
      "segs": [
        {
          "utf8": "already know the physics in an environment or if we already know the"
        }
      ]
    },
    {
      "tStartMs": 5120040,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "rules of a board game or it can be learned by the agent at the same time"
        }
      ]
    },
    {
      "tStartMs": 5125000,
      "dDurationMs": 4280,
      "segs": [
        {
          "utf8": "that it learns a policy or value function and then once you have the"
        }
      ]
    },
    {
      "tStartMs": 5129280,
      "dDurationMs": 6480,
      "segs": [
        {
          "utf8": "world model you can also use it in two main ways you can either just train as"
        }
      ]
    },
    {
      "tStartMs": 5135760,
      "dDurationMs": 4879,
      "segs": [
        {
          "utf8": "usual but with imagined simulated experiences from the world's model kind"
        }
      ]
    },
    {
      "tStartMs": 5140639,
      "dDurationMs": 3921,
      "segs": [
        {
          "utf8": "of like that study which found that people were able to improve at shooting"
        }
      ]
    },
    {
      "tStartMs": 5144560,
      "dDurationMs": 5200,
      "segs": [
        {
          "utf8": "free throws by imagining themselves shooting free throws or you can use the"
        }
      ]
    },
    {
      "tStartMs": 5149760,
      "dDurationMs": 5200,
      "segs": [
        {
          "utf8": "world model to plan ahead while you're actually doing the task by simulating"
        }
      ]
    },
    {
      "tStartMs": 5154960,
      "dDurationMs": 4440,
      "segs": [
        {
          "utf8": "different possibilities and searching for the best option a way of doing this"
        }
      ]
    },
    {
      "tStartMs": 5159400,
      "dDurationMs": 6200,
      "segs": [
        {
          "utf8": "is Monte Carlo tree search Monty Carlo meaning simulated Tree Search meaning"
        }
      ]
    },
    {
      "tStartMs": 5165600,
      "dDurationMs": 4240,
      "segs": [
        {
          "utf8": "considering how different possibilities of the future may Branch out and"
        }
      ]
    },
    {
      "tStartMs": 5169840,
      "dDurationMs": 4279,
      "segs": [
        {
          "utf8": "searching for the best one Google Deep Mind has some good examples of"
        }
      ]
    },
    {
      "tStartMs": 5174119,
      "dDurationMs": 7201,
      "segs": [
        {
          "utf8": "model-based RL methods developing over time from alphao to Alpha go0 to alpha0"
        }
      ]
    },
    {
      "tStartMs": 5181320,
      "dDurationMs": 6120,
      "segs": [
        {
          "utf8": "to mu0 over time they've made the able to do more and more tasks expanding from"
        }
      ]
    },
    {
      "tStartMs": 5187440,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "just go to other board games like chess and shogi and then to Atari games as"
        }
      ]
    },
    {
      "tStartMs": 5192800,
      "dDurationMs": 5800,
      "segs": [
        {
          "utf8": "well the first three are given a model and only use the model for planning the"
        }
      ]
    },
    {
      "tStartMs": 5198600,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "last one muz learns a model by itself and uses the model for both learning"
        }
      ]
    },
    {
      "tStartMs": 5204600,
      "dDurationMs": 6360,
      "segs": [
        {
          "utf8": "from imagined experiences and planning another example is the dreamer series"
        }
      ]
    },
    {
      "tStartMs": 5210960,
      "dDurationMs": 5000,
      "segs": [
        {
          "utf8": "with dreamer and then dreamer V2 and then dreamer V3 three these ones learn a"
        }
      ]
    },
    {
      "tStartMs": 5215960,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "model by themselves and use the model only for learning from imagined"
        }
      ]
    },
    {
      "tStartMs": 5220760,
      "dDurationMs": 6399,
      "segs": [
        {
          "utf8": "experiences and not for planning the most recent version dreamer V3 was able"
        }
      ]
    },
    {
      "tStartMs": 5227159,
      "dDurationMs": 6241,
      "segs": [
        {
          "utf8": "to obtain diamonds in a modified version of Minecraft so that is very cool as you"
        }
      ]
    },
    {
      "tStartMs": 5233400,
      "dDurationMs": 4279,
      "segs": [
        {
          "utf8": "may imagine world's models are very complicated and advanced so we won't go"
        }
      ]
    },
    {
      "tStartMs": 5237679,
      "dDurationMs": 5321,
      "segs": [
        {
          "utf8": "into it too much here the second key fields of RL actually two closely"
        }
      ]
    },
    {
      "tStartMs": 5243000,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "related fields with a similar goal is imitation learning and inverse"
        }
      ]
    },
    {
      "tStartMs": 5248440,
      "dDurationMs": 4880,
      "segs": [
        {
          "utf8": "reinforcement learning which aim to learn from a so-called experts"
        }
      ]
    },
    {
      "tStartMs": 5253320,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "demonstrations if we think about many day-to-day tasks such as tying shoelaces"
        }
      ]
    },
    {
      "tStartMs": 5258360,
      "dDurationMs": 4480,
      "segs": [
        {
          "utf8": "folding a shirts or even shooting a basketball almost all of them are"
        }
      ]
    },
    {
      "tStartMs": 5262840,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "usually learned by watching someone else demonstrate you wouldn't expect a"
        }
      ]
    },
    {
      "tStartMs": 5267560,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "3-year-old to learn how to tie shoelaces just through trial and error so if we"
        }
      ]
    },
    {
      "tStartMs": 5272280,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "want robots to one day learn tasks of similar complexity it would be great if"
        }
      ]
    },
    {
      "tStartMs": 5277560,
      "dDurationMs": 4240,
      "segs": [
        {
          "utf8": "we could somehow teach through demonstration the earliest attempts to"
        }
      ]
    },
    {
      "tStartMs": 5281800,
      "dDurationMs": 4399,
      "segs": [
        {
          "utf8": "achieve this goal were through something called imitation learning which is"
        }
      ]
    },
    {
      "tStartMs": 5286199,
      "dDurationMs": 5801,
      "segs": [
        {
          "utf8": "trying to learn a policy a mapping from states to actions not from a reward"
        }
      ]
    },
    {
      "tStartMs": 5292000,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "signal but just from example trajectories from someone who supposedly"
        }
      ]
    },
    {
      "tStartMs": 5297280,
      "dDurationMs": 4959,
      "segs": [
        {
          "utf8": "knows what they're doing so for example if I want to teach an agent to drive a"
        }
      ]
    },
    {
      "tStartMs": 5302239,
      "dDurationMs": 4321,
      "segs": [
        {
          "utf8": "car around a racetrack I'll just drive it around the racetrack myself a couple"
        }
      ]
    },
    {
      "tStartMs": 5306560,
      "dDurationMs": 5679,
      "segs": [
        {
          "utf8": "times and then record the states and actions that occurred as training data"
        }
      ]
    },
    {
      "tStartMs": 5312239,
      "dDurationMs": 4561,
      "segs": [
        {
          "utf8": "now notice how there's no need to design a reward function here which is actually"
        }
      ]
    },
    {
      "tStartMs": 5316800,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "a pretty great feature because designing reward functions can actually be quite"
        }
      ]
    },
    {
      "tStartMs": 5321760,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "difficult to get the agent to do exactly what you want it to do for example if"
        }
      ]
    },
    {
      "tStartMs": 5326800,
      "dDurationMs": 4040,
      "segs": [
        {
          "utf8": "I'm not careful and I just give the agents a reward for passing the Finish"
        }
      ]
    },
    {
      "tStartMs": 5330840,
      "dDurationMs": 4440,
      "segs": [
        {
          "utf8": "Line it might end up just going back and forth across that finish line to collect"
        }
      ]
    },
    {
      "tStartMs": 5335280,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "reward instead of actually going around the racetrack and that's not what I"
        }
      ]
    },
    {
      "tStartMs": 5339440,
      "dDurationMs": 4880,
      "segs": [
        {
          "utf8": "wanted to do if we think about an even more complex task of just generally"
        }
      ]
    },
    {
      "tStartMs": 5344320,
      "dDurationMs": 4520,
      "segs": [
        {
          "utf8": "driving safely changing lanes and signaling turns Etc how do you even"
        }
      ]
    },
    {
      "tStartMs": 5348840,
      "dDurationMs": 4480,
      "segs": [
        {
          "utf8": "Define the reward function for that these are examples of where an expert's"
        }
      ]
    },
    {
      "tStartMs": 5353320,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "demonstration could be greatly useful anyways without the reward function the"
        }
      ]
    },
    {
      "tStartMs": 5358600,
      "dDurationMs": 5559,
      "segs": [
        {
          "utf8": "simplest way of learning the behavior within imitation learning is to learn a"
        }
      ]
    },
    {
      "tStartMs": 5364159,
      "dDurationMs": 4761,
      "segs": [
        {
          "utf8": "policy just from the generated trajectories such as with a table or a"
        }
      ]
    },
    {
      "tStartMs": 5368920,
      "dDurationMs": 3960,
      "segs": [
        {
          "utf8": "neural network and this is called behavioral"
        }
      ]
    },
    {
      "tStartMs": 5372880,
      "dDurationMs": 5239,
      "segs": [
        {
          "utf8": "cloning however the issue with that is it works fine when the agent is able to"
        }
      ]
    },
    {
      "tStartMs": 5378119,
      "dDurationMs": 5080,
      "segs": [
        {
          "utf8": "perfectly follow the example trajectory but if it veers ever so slightly off"
        }
      ]
    },
    {
      "tStartMs": 5383199,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "course it won't know what to do because the examples didn't cover that and it'll"
        }
      ]
    },
    {
      "tStartMs": 5388639,
      "dDurationMs": 4641,
      "segs": [
        {
          "utf8": "just keep deviating more and more to solve that there's an improvement upon"
        }
      ]
    },
    {
      "tStartMs": 5393280,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "behavioral Clon learning which is still within the realm of imitation learning"
        }
      ]
    },
    {
      "tStartMs": 5397840,
      "dDurationMs": 5799,
      "segs": [
        {
          "utf8": "called data set aggregation or dagger for short machine learning people do"
        }
      ]
    },
    {
      "tStartMs": 5403639,
      "dDurationMs": 4721,
      "segs": [
        {
          "utf8": "love coming up with Goofy acronyms in this case the agents will purposefully"
        }
      ]
    },
    {
      "tStartMs": 5408360,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "add some noise to the example trajectories to generate instances where"
        }
      ]
    },
    {
      "tStartMs": 5413320,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "it has veered off course of bit and then a human has to go and annotate what it"
        }
      ]
    },
    {
      "tStartMs": 5417880,
      "dDurationMs": 4680,
      "segs": [
        {
          "utf8": "should do in those instances this makes for a more robust type of imitation"
        }
      ]
    },
    {
      "tStartMs": 5422560,
      "dDurationMs": 4559,
      "segs": [
        {
          "utf8": "learning but of course the downside is it's very tedious to have to have humans"
        }
      ]
    },
    {
      "tStartMs": 5427119,
      "dDurationMs": 4921,
      "segs": [
        {
          "utf8": "go and label all that data despite that imitation learning has been used to do"
        }
      ]
    },
    {
      "tStartMs": 5432040,
      "dDurationMs": 5000,
      "segs": [
        {
          "utf8": "some pretty cool stuff for example this paper shows a human demonstrating to a"
        }
      ]
    },
    {
      "tStartMs": 5437040,
      "dDurationMs": 5000,
      "segs": [
        {
          "utf8": "robot how to catch a ball in a cup and you can see that at first when the robot"
        }
      ]
    },
    {
      "tStartMs": 5442040,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "tries to imitate that it's not successful but over time it gets closer"
        }
      ]
    },
    {
      "tStartMs": 5446600,
      "dDurationMs": 5119,
      "segs": [
        {
          "utf8": "and closer and finally after a 100 trials is able to catch the ball in the"
        }
      ]
    },
    {
      "tStartMs": 5451719,
      "dDurationMs": 5201,
      "segs": [
        {
          "utf8": "cup I also found this example of a robot dog learning to walk by imitating a real"
        }
      ]
    },
    {
      "tStartMs": 5456920,
      "dDurationMs": 5600,
      "segs": [
        {
          "utf8": "dog and a surgery robot learning to suture by imitating human footage in"
        }
      ]
    },
    {
      "tStartMs": 5462520,
      "dDurationMs": 6119,
      "segs": [
        {
          "utf8": "contrast to imitation learning inverse reinforcement learning or inverse RL"
        }
      ]
    },
    {
      "tStartMs": 5468639,
      "dDurationMs": 4441,
      "segs": [
        {
          "utf8": "refers to methods that rather try to learn what the reward function is"
        }
      ]
    },
    {
      "tStartMs": 5473080,
      "dDurationMs": 4520,
      "segs": [
        {
          "utf8": "supposed to be it's kind of like observing some behavior and rather than"
        }
      ]
    },
    {
      "tStartMs": 5477600,
      "dDurationMs": 5039,
      "segs": [
        {
          "utf8": "trying to mindlessly follow it actually try to figure out what the underlying"
        }
      ]
    },
    {
      "tStartMs": 5482639,
      "dDurationMs": 5961,
      "segs": [
        {
          "utf8": "goal is and what outcomes are good or bad I didn't find as many cool demos for"
        }
      ]
    },
    {
      "tStartMs": 5488600,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "inverse RL as for imitation learning but one paper showed a method called inverse"
        }
      ]
    },
    {
      "tStartMs": 5493960,
      "dDurationMs": 5520,
      "segs": [
        {
          "utf8": "Q learning which focuses on learning the Q function from expert demonstration and"
        }
      ]
    },
    {
      "tStartMs": 5499480,
      "dDurationMs": 5120,
      "segs": [
        {
          "utf8": "subsequently the reward function the agent seems to be able to play Atari"
        }
      ]
    },
    {
      "tStartMs": 5504600,
      "dDurationMs": 5680,
      "segs": [
        {
          "utf8": "games perform movement in Minecraft control a simulated robot arm Etc just"
        }
      ]
    },
    {
      "tStartMs": 5510280,
      "dDurationMs": 5959,
      "segs": [
        {
          "utf8": "from watching 20 to 30 expert demos also pretty"
        }
      ]
    },
    {
      "tStartMs": 5518560,
      "dDurationMs": 3089,
      "segs": [
        {
          "utf8": "[Music]"
        }
      ]
    },
    {
      "tStartMs": 5521960,
      "dDurationMs": 5719,
      "segs": [
        {
          "utf8": "cool so there we have it to recap this video we first went through what a mark"
        }
      ]
    },
    {
      "tStartMs": 5527679,
      "dDurationMs": 5241,
      "segs": [
        {
          "utf8": "of decision process is and then applied it to a grid maze example we solved this"
        }
      ]
    },
    {
      "tStartMs": 5532920,
      "dDurationMs": 4000,
      "segs": [
        {
          "utf8": "grid maze example with a Monte Carlo method and then showed how we can"
        }
      ]
    },
    {
      "tStartMs": 5536920,
      "dDurationMs": 3960,
      "segs": [
        {
          "utf8": "improve upon that with temporal difference using a method called Q"
        }
      ]
    },
    {
      "tStartMs": 5540880,
      "dDurationMs": 5200,
      "segs": [
        {
          "utf8": "learning then we added on a neural network to to Q learning making deep Q"
        }
      ]
    },
    {
      "tStartMs": 5546080,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "networks in order to learn lunar lander now going from discreet to continuous"
        }
      ]
    },
    {
      "tStartMs": 5551440,
      "dDurationMs": 5880,
      "segs": [
        {
          "utf8": "State spaces after that we went through the mass of policy gradients and with"
        }
      ]
    },
    {
      "tStartMs": 5557320,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "policy gradients we were also able to handle continuous action spaces not just"
        }
      ]
    },
    {
      "tStartMs": 5562280,
      "dDurationMs": 4320,
      "segs": [
        {
          "utf8": "continuous State spaces then we talked about how aspects"
        }
      ]
    },
    {
      "tStartMs": 5566600,
      "dDurationMs": 3559,
      "segs": [
        {
          "utf8": "of reinforcement learning show correlations to neuroscience and certain"
        }
      ]
    },
    {
      "tStartMs": 5570159,
      "dDurationMs": 3960,
      "segs": [
        {
          "utf8": "parts of the brain and finally we discussed the current shortcom comings"
        }
      ]
    },
    {
      "tStartMs": 5574119,
      "dDurationMs": 4520,
      "segs": [
        {
          "utf8": "of RL and what directions of future research could help with it I hope you"
        }
      ]
    },
    {
      "tStartMs": 5578639,
      "dDurationMs": 4681,
      "segs": [
        {
          "utf8": "found value from this video if you did and you're interested in more videos on"
        }
      ]
    },
    {
      "tStartMs": 5583320,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "related topics of AI and Robotics consider supporting the channel on"
        }
      ]
    },
    {
      "tStartMs": 5587480,
      "dDurationMs": 5560,
      "segs": [
        {
          "utf8": "patreon where I'll put behind the scenes clips and previews Link in description I"
        }
      ]
    },
    {
      "tStartMs": 5593040,
      "dDurationMs": 3199,
      "segs": [
        {
          "utf8": "hope you learned a lot about reinforcement learning I hope you found"
        }
      ]
    },
    {
      "tStartMs": 5596239,
      "dDurationMs": 4480,
      "segs": [
        {
          "utf8": "the field fascinating and I hope one day we will have robots that can tie our"
        }
      ]
    },
    {
      "tStartMs": 5600719,
      "dDurationMs": 6881,
      "segs": [
        {
          "utf8": "shoes and fold our clothes with that being said see you next time"
        }
      ]
    }
  ],
  "pens": [
    {}
  ],
  "wireMagic": "pb3",
  "wpWinPositions": [
    {},
    {
      "apPoint": 6,
      "ahHorPos": 20,
      "avVerPos": 100,
      "rcRows": 2,
      "ccCols": 40
    }
  ],
  "wsWinStyles": [
    {},
    {
      "mhModeHint": 2,
      "juJustifCode": 0,
      "sdScrollDir": 3
    },
    {
      "mhModeHint": 2,
      "juJustifCode": 1,
      "sdScrollDir": 3
    }
  ]
}